{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome I am an infrastructure engineer working on the University of Cambridge Dawn Supercomputer, specialising in deploying datacenter and cloud infrastructure and managing automation of servers, virtual machines, containers and networks. I have put this site together mostly as a note-taking practice for my own reference, but if you happen to stumble across it, I hope you find something useful for yourself in here. Services I am also availble to offer technical advice for anyone hosting their own services in the cloud or on-premise, either through consultation, or general technical support. Please get in touch if you would like support with any of the following: Deploying AI models on-prem or in the cloud Setting up cloud/hybrid/on-prem network infrastructure Auto-scaling solutions in the cloud Adding/Improving observability and monitoring of any infrastructure Diagnosing issues with cloud/on-prem/hybrid networks Diagnosing issues with services running on Docker or K8s Hardware migrations or system upgrades You can contact me at: awnet.uk@gmail.com","title":"Home"},{"location":"#welcome","text":"I am an infrastructure engineer working on the University of Cambridge Dawn Supercomputer, specialising in deploying datacenter and cloud infrastructure and managing automation of servers, virtual machines, containers and networks. I have put this site together mostly as a note-taking practice for my own reference, but if you happen to stumble across it, I hope you find something useful for yourself in here.","title":"Welcome"},{"location":"#services","text":"I am also availble to offer technical advice for anyone hosting their own services in the cloud or on-premise, either through consultation, or general technical support. Please get in touch if you would like support with any of the following: Deploying AI models on-prem or in the cloud Setting up cloud/hybrid/on-prem network infrastructure Auto-scaling solutions in the cloud Adding/Improving observability and monitoring of any infrastructure Diagnosing issues with cloud/on-prem/hybrid networks Diagnosing issues with services running on Docker or K8s Hardware migrations or system upgrades You can contact me at: awnet.uk@gmail.com","title":"Services"},{"location":"HPC/AWS_parallel_cluster/","text":"What is Parallel Cluster Parallel cluster is an open-source tool provided by AWS for provisioning on-demand HPC clusters in AWS. Install Parallel Cluster Parallel Cluster is a pip package which can be installed with: pip install aws-parallelcluster --user This installation provides access to the pcluster CLI Cluster Configuration When creating a cluster, a cluster definition file is needed which will define the head node, access to the cluster, network configuration and scheduling tools. Head Node Region : us-east-1 Image : Os : alinux2 HeadNode : InstanceType : c6i.2xlarge Networking : SubnetId : $SUBNET_ID Ssh : KeyName : ws-default-keypair LocalStorage : RootVolume : VolumeType : gp3 Dcv : Enabled : true Port : 8443 AllowedIps : 0.0.0.0/0 Iam : AdditionalIamPolicies : - Policy : arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore - Policy : arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Imds : Secured : false Scheduler Scheduling : Scheduler : slurm SlurmQueues : - Name : c6i ComputeResources : - Name : compute Instances : - InstanceType : c6i.32xlarge MinCount : 0 MaxCount : 2 Efa : Enabled : true GdrSupport : true Networking : SubnetIds : - $SUBNET_ID PlacementGroup : Enabled : true Storage You can configure the cluster to mount FSx file systems for Lustre storage access. SharedStorage : - Name : FSx StorageType : FsxLustre MountDir : /fsx FsxLustreSettings : FileSystemId : $FSX_FS_ID Combine this all together into a cluster.yaml file Create a Cluster Using the cluster configuration file from previously, the defined cluster can be provisioned with the pcluster CLI. pcluster create-cluster --cluster-name $CLUSTER_NAME --cluster-configuration cluster.yaml Created clusters can be viewed with pcluster list-clusters FSx FSx provides a fast storage using Lustre, which can be attached to parallel cluster nodes for optimised HPC workloads. Data Repository Association A DRA needs to be setup between an FSx and an S3 bucket. aws fsx create - data - repository - association \\ -- file - system - id $FSX_ID \\ -- file - system - path \"/dra\" \\ -- data - repository - path s3: //$S3_BUCKET_NAME \\ --s3 AutoImportPolicy='{Events=[NEW,CHANGED,DELETED]},AutoExportPolicy={Events=[NEW,CHANGED,DELETED]}' \\ --batch-import-meta-data-on-create \\ --region $AWS_REGION Once the DRA is created, from the head node of the cluster, the dra directory should be visible under /fsx . Files uploaded to the S3 bucket will be visible from this directory however, they are lazy loaded. Lazy Loading Lazy loading means that the metadata is visible from the directory but the data is only copied from the upstream S3 bucket at the time of first access. If you run lfs hsm_state /fsx/dra/$FILE against an uploaded file in the S3 bucket, the state will show as released . You can verify that subsequent accesses to the file are faster by cat-ing the file time cat /fsx/dra/$FILE > /dev/shm/fsx time cat /fsx/dra/$FILE > /dev/shm/fsx The hsm_state will now show as archived and the Lustre lfs df -h command will now show the full file size. To release it from the system run lfs hsm_release /fsx/dra/$FILE . Auto-Export The DRA configured previously also setup an AutoExportPolicy which tells the FSx filesystem to automatically update the state of the S3 bucket with any new, changed or deleted files.","title":"AWS parallel cluster"},{"location":"HPC/AWS_parallel_cluster/#what-is-parallel-cluster","text":"Parallel cluster is an open-source tool provided by AWS for provisioning on-demand HPC clusters in AWS.","title":"What is Parallel Cluster"},{"location":"HPC/AWS_parallel_cluster/#install-parallel-cluster","text":"Parallel Cluster is a pip package which can be installed with: pip install aws-parallelcluster --user This installation provides access to the pcluster CLI","title":"Install Parallel Cluster"},{"location":"HPC/AWS_parallel_cluster/#cluster-configuration","text":"When creating a cluster, a cluster definition file is needed which will define the head node, access to the cluster, network configuration and scheduling tools.","title":"Cluster Configuration"},{"location":"HPC/AWS_parallel_cluster/#head-node","text":"Region : us-east-1 Image : Os : alinux2 HeadNode : InstanceType : c6i.2xlarge Networking : SubnetId : $SUBNET_ID Ssh : KeyName : ws-default-keypair LocalStorage : RootVolume : VolumeType : gp3 Dcv : Enabled : true Port : 8443 AllowedIps : 0.0.0.0/0 Iam : AdditionalIamPolicies : - Policy : arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore - Policy : arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Imds : Secured : false","title":"Head Node"},{"location":"HPC/AWS_parallel_cluster/#scheduler","text":"Scheduling : Scheduler : slurm SlurmQueues : - Name : c6i ComputeResources : - Name : compute Instances : - InstanceType : c6i.32xlarge MinCount : 0 MaxCount : 2 Efa : Enabled : true GdrSupport : true Networking : SubnetIds : - $SUBNET_ID PlacementGroup : Enabled : true","title":"Scheduler"},{"location":"HPC/AWS_parallel_cluster/#storage","text":"You can configure the cluster to mount FSx file systems for Lustre storage access. SharedStorage : - Name : FSx StorageType : FsxLustre MountDir : /fsx FsxLustreSettings : FileSystemId : $FSX_FS_ID Combine this all together into a cluster.yaml file","title":"Storage"},{"location":"HPC/AWS_parallel_cluster/#create-a-cluster","text":"Using the cluster configuration file from previously, the defined cluster can be provisioned with the pcluster CLI. pcluster create-cluster --cluster-name $CLUSTER_NAME --cluster-configuration cluster.yaml Created clusters can be viewed with pcluster list-clusters","title":"Create a Cluster"},{"location":"HPC/AWS_parallel_cluster/#fsx","text":"FSx provides a fast storage using Lustre, which can be attached to parallel cluster nodes for optimised HPC workloads.","title":"FSx"},{"location":"HPC/AWS_parallel_cluster/#data-repository-association","text":"A DRA needs to be setup between an FSx and an S3 bucket. aws fsx create - data - repository - association \\ -- file - system - id $FSX_ID \\ -- file - system - path \"/dra\" \\ -- data - repository - path s3: //$S3_BUCKET_NAME \\ --s3 AutoImportPolicy='{Events=[NEW,CHANGED,DELETED]},AutoExportPolicy={Events=[NEW,CHANGED,DELETED]}' \\ --batch-import-meta-data-on-create \\ --region $AWS_REGION Once the DRA is created, from the head node of the cluster, the dra directory should be visible under /fsx . Files uploaded to the S3 bucket will be visible from this directory however, they are lazy loaded.","title":"Data Repository Association"},{"location":"HPC/AWS_parallel_cluster/#lazy-loading","text":"Lazy loading means that the metadata is visible from the directory but the data is only copied from the upstream S3 bucket at the time of first access. If you run lfs hsm_state /fsx/dra/$FILE against an uploaded file in the S3 bucket, the state will show as released . You can verify that subsequent accesses to the file are faster by cat-ing the file time cat /fsx/dra/$FILE > /dev/shm/fsx time cat /fsx/dra/$FILE > /dev/shm/fsx The hsm_state will now show as archived and the Lustre lfs df -h command will now show the full file size. To release it from the system run lfs hsm_release /fsx/dra/$FILE .","title":"Lazy Loading"},{"location":"HPC/AWS_parallel_cluster/#auto-export","text":"The DRA configured previously also setup an AutoExportPolicy which tells the FSx filesystem to automatically update the state of the S3 bucket with any new, changed or deleted files.","title":"Auto-Export"},{"location":"HPC/slurm_basics/","text":"Slurm Basics Commands # Show cluster information sinfo # Show jobs in queue squeue # Submit script to cluster sbatch FILE . sbatch Job Submission Create an sbatch file to be submitted to Slurm: #!/bin/bash #SBATCH --job-name=ior #SBATCH --ntasks=10 #SBATCH --output=%x.out module load intelmpi mpirun ./ior -w -r -o = /shared/test_dir -b = 256m -a = POSIX -i = 5 -F -z -t = 64m -C The script above makes use ior which is an IO performance tool for parallel file systems. This is assuming a host running Intel MPI.","title":"Slurm basics"},{"location":"HPC/slurm_basics/#slurm-basics","text":"","title":"Slurm Basics"},{"location":"HPC/slurm_basics/#commands","text":"# Show cluster information sinfo # Show jobs in queue squeue # Submit script to cluster sbatch FILE . sbatch","title":"Commands"},{"location":"HPC/slurm_basics/#job-submission","text":"Create an sbatch file to be submitted to Slurm: #!/bin/bash #SBATCH --job-name=ior #SBATCH --ntasks=10 #SBATCH --output=%x.out module load intelmpi mpirun ./ior -w -r -o = /shared/test_dir -b = 256m -a = POSIX -i = 5 -F -z -t = 64m -C The script above makes use ior which is an IO performance tool for parallel file systems. This is assuming a host running Intel MPI.","title":"Job Submission"},{"location":"HPC/spack/","text":"Why use Spack? Reproducibility Spack makes advancements on the traditional binary package managers like apt and yum by allowing a system to support multiple different versions and dependency trees. Spack abstracts away a lot of the complexity of pacakage management on HPC systems and allows for easy reproducibility of research software runtime environments. On top of this, it provides a much more friendly CLI for managing these dependency trees of varying complexity. Spack ensures reproducibility by tying dependency trees and linking packages to their dependent compilers, so environments can be rebuilt in a consistent way. It can also speed up installations by aiming to build dependency trees based on what the machine already has. While reproducibility can be solved by tools like Virtual Machine Images and Docker, Spack attempts to handle the reproducibility on the package level, without having to make an entire copy of the system. Compiler and Architecture Considerations Spack ensures that packages being installed a compatible with the host and relevant to the compiler version and architectures avilable. All libraries in a depency chain should use compatible compilers to ensure ABI and linking compatibility. Environments Environments are cirtualised spack instances that can be used to aggregate package installations for a project. spack env create myenv spack env activate myenv # Add a package into an environment spack add tcl spack install # Remove a package from an environment spack remove tcl # Concretize the environment spack concretize spack env deactivate It is still possible to use globally installed packages within an environment, but commands like spack find will show no installed packaages. Spack environments will spit out a spack.lock which can be shared for reproducibility. Mirrors Spack mirrors are designed to host local repositories where hosts can get their files from. This is used in the event of the host not having access to the internet or the site where the packages are typically installed from. Concretization Concretization fills out any missing configuration details on a dependency graph when a user is not explicit. This will then explicitly set versions, compilers, architectures and compiler versions for each package in the dependency graph for guaranteed reproducibility. Complexity Handling Spack ensures that multiple different versions and variations can be installed by taking a hash of the dependency graph for each installed package. The hash is then appended to each package prefix and saved under the relevant compiler in the spack directory. This ensures that each unique dependency graph is a unique configuration meaning multiple configurations of the same package can coexist on the system. External Software Spack packages can be built with external software so not all packages need to be spack packages. The main issue with using external software is that the DAG gets pruned meaning Spack does not know anything about the external software below it in the tree. External software can be configured in the packages.yaml , specifying the path to where the external software can be found. Commands # List all packages from repository spack list # Show installed packages and different versions spack find # Show spack compilers spack compilers # Configure a spack mirror spack mirror mymirror / mirror # Find location of package spack location - i gcc @12 # Specify the compiler for the install spack install gmake % clang # Explicitly specify the version of a dependency spack install tcl ^ zlib - ng @2.0.7 % clang # Install package from package has spack install tcl ^/ 6 bh # Disable a dependency spack install hdf5 ~ mpi # Graphical view of dependencies spack graph # Uninstall pacakages (Remove zlib-ng built with gcc version 10) spack uninstall zlib - ng % gcc @10 # Spack will attempt to prevent uninstalling pacakages that are used as a dependency # Uninstall dependents along with package spack uninstall -- dependents zlib - ng / $HASH # Target a specific architecture spack install mpileaks @3.3 target = cascadelake # Get spack config spack config get","title":"Spack"},{"location":"HPC/spack/#why-use-spack","text":"","title":"Why use Spack?"},{"location":"HPC/spack/#reproducibility","text":"Spack makes advancements on the traditional binary package managers like apt and yum by allowing a system to support multiple different versions and dependency trees. Spack abstracts away a lot of the complexity of pacakage management on HPC systems and allows for easy reproducibility of research software runtime environments. On top of this, it provides a much more friendly CLI for managing these dependency trees of varying complexity. Spack ensures reproducibility by tying dependency trees and linking packages to their dependent compilers, so environments can be rebuilt in a consistent way. It can also speed up installations by aiming to build dependency trees based on what the machine already has. While reproducibility can be solved by tools like Virtual Machine Images and Docker, Spack attempts to handle the reproducibility on the package level, without having to make an entire copy of the system.","title":"Reproducibility"},{"location":"HPC/spack/#compiler-and-architecture-considerations","text":"Spack ensures that packages being installed a compatible with the host and relevant to the compiler version and architectures avilable. All libraries in a depency chain should use compatible compilers to ensure ABI and linking compatibility.","title":"Compiler and Architecture Considerations"},{"location":"HPC/spack/#environments","text":"Environments are cirtualised spack instances that can be used to aggregate package installations for a project. spack env create myenv spack env activate myenv # Add a package into an environment spack add tcl spack install # Remove a package from an environment spack remove tcl # Concretize the environment spack concretize spack env deactivate It is still possible to use globally installed packages within an environment, but commands like spack find will show no installed packaages. Spack environments will spit out a spack.lock which can be shared for reproducibility.","title":"Environments"},{"location":"HPC/spack/#mirrors","text":"Spack mirrors are designed to host local repositories where hosts can get their files from. This is used in the event of the host not having access to the internet or the site where the packages are typically installed from.","title":"Mirrors"},{"location":"HPC/spack/#concretization","text":"Concretization fills out any missing configuration details on a dependency graph when a user is not explicit. This will then explicitly set versions, compilers, architectures and compiler versions for each package in the dependency graph for guaranteed reproducibility.","title":"Concretization"},{"location":"HPC/spack/#complexity-handling","text":"Spack ensures that multiple different versions and variations can be installed by taking a hash of the dependency graph for each installed package. The hash is then appended to each package prefix and saved under the relevant compiler in the spack directory. This ensures that each unique dependency graph is a unique configuration meaning multiple configurations of the same package can coexist on the system.","title":"Complexity Handling"},{"location":"HPC/spack/#external-software","text":"Spack packages can be built with external software so not all packages need to be spack packages. The main issue with using external software is that the DAG gets pruned meaning Spack does not know anything about the external software below it in the tree. External software can be configured in the packages.yaml , specifying the path to where the external software can be found.","title":"External Software"},{"location":"HPC/spack/#commands","text":"# List all packages from repository spack list # Show installed packages and different versions spack find # Show spack compilers spack compilers # Configure a spack mirror spack mirror mymirror / mirror # Find location of package spack location - i gcc @12 # Specify the compiler for the install spack install gmake % clang # Explicitly specify the version of a dependency spack install tcl ^ zlib - ng @2.0.7 % clang # Install package from package has spack install tcl ^/ 6 bh # Disable a dependency spack install hdf5 ~ mpi # Graphical view of dependencies spack graph # Uninstall pacakages (Remove zlib-ng built with gcc version 10) spack uninstall zlib - ng % gcc @10 # Spack will attempt to prevent uninstalling pacakages that are used as a dependency # Uninstall dependents along with package spack uninstall -- dependents zlib - ng / $HASH # Target a specific architecture spack install mpileaks @3.3 target = cascadelake # Get spack config spack config get","title":"Commands"},{"location":"electrical/UK_Qualification_Requirements/","text":"In order to be classified as a fully qualified electrician, there are multiple different criteria you need to meet and skills you need to be able to demonstrate. I am doing my best to provide a more detailed page on each of the practical assessments. If you decide to go down the electrician certification route as well, I hope some of the blogs on this site will help provide some guidance. Level 2 Level 2 is targetted mostly at domestic installations. Practical Exams There are 2 practical exams at level 2; one for inspection and testing, another for building containment systems. The inspection and testing exam will require you to conduct testing in the correct sequence on a radial, ring and lighting installation; while the containment exam will consist of constructing various containment systems from PVC and metal conduit, to cable tray and metal trunking. All other exams at level 2 are multiple choice, covering topics like health and safety, BS7671, Building Regulations and Electrical Science BS7671 One of the hardest exams at level 2 is the BS7671, which is a multiple choice exam which entails navigating the BS7671 regulations to the find answers to the questions. It will require a good amount of confidence in navigating the regulations. Building Regulations The building regulations exam will require you to find information from the Building Regulations for Electricians book. It summarises all the different sections of the building regulations and goes into details about Part P (The section which refers to electrical work). The questions will cover topics such as how much space to leave when drilling into joists and what areas are considered safe zones for electrical cabling to be run without additional protection. Although this covers Part P, completing this exam does not mean that you are Part P qualified. Electrical Science Electrical science covers some basic math that applies to electrical work. This includes calculating resistance in series and parallel as well as magnetic flux and power calculations. Level 3 When training for level 3, I was required to attend a training center for 3 separate weeks, compared to level 2 which can be achieved over multiple weekends. Level 3 Exams Level 3 exams include a mix of multiple choice and written exams. One of the written exams include a slightly more challenging electrical science (compared to level 2), this will require a good knowledge of AC motors and the wiring of different motor starter types. It also covers more advanced electrical science principals such as reactance (ICE and ELI). The other exams are around inspection and testing, and environmental technology. On top of the written and multiple choice exams, there are 3 practical exams to complete. Fault Finding Fault finding is a practical assessment where you are required to identify different fault types and where exactly they are in a circuit. The examiner will report either a piece of equipment not working (open circuit), or a breaker keep on tripping (short circuit). You are required to use your knowledge from inspection and testing to conduct the appropriate test, which should provide the answer of whether it is a short circuit or an open circuit. You will need to specify exactly where the open/short circuit is, and remember that a light bulb can also contribute to the open circuit... Inspection and Testing The inspection and testing is similar to the practical exam of level 2, requiring you to show competence inspecting an installation, safely isolating the circuit, and carrying out a full testing sequence of all the circuits. On completion, you will then need to complete an Electrical Installation Certificate (EIC) with all your test results and state whether the installation is satisfactory based on your results and if the Zs results are satisfactory against the tabulated values in BS7671. Circuit Design Circuit design is arguably one of the most challenging weeks in level 3. It collates together all the electrical knowledge and electrical science components up until this week. You are provided with a floorplan and a specification document stating the requirements of the installation and given all week (you will need to do some work out of hours) to complete an entire installation plan. Throughout the assignment, the assessor will act as the client of the installation and provide feedback as well as challenge some of the decisions you have made. Firstly, you will need to find the different equipment to go into your installation. Using a site like ScreFix is useful for this, as you can add all the components into your basket as you go, so it is easy to go back and refer to them if needed. The equipment will include multiple 3 phase machines, heating systems, lighting (emergency + non-emergency and needs to be appropriate for floorplan), isolators/switches, socket outlets, circuit breakers, kitchen equipment and anything else appropriate to the specification. Shopping aside, you will need to make note of their power requirements, as you will need this for cable size calculations later. Next you need to create wiring diagrams for all circuits in the installation with correct symbols for equipment and show whether they are single phase or 3 phase circuits, maintained or non-maintained emergency lighting, ring or radial circuits etc.. Using these to-scale diagrams, you can determine the length of wiring needed for the circuit. Using the power ratings of the selected equipment, the length of the circuits and the installation method, you can start calculating the cable size and breaker size requirements referring to BS7671. I highly recommend using a spreadsheet for all this as there are a lot of circuits and a lot of different numbers. You may find you need to change equipment over if it is too power hungry or split the circuits out if all the equipment draws too much power. With all the cable calculations complete and cable sizes determined, you will need to assign the circuits to a 3-phase board location. You will need to apply diversification to your circuits based on information in the On-Site Guide and then assign each circuit to either an L1, L2 or L3 position on the board. You will need to demonstrate that the load is appropriately balanced across the 3 phases. NVQ On comlpetion of level 3, you should be ready to begin your NVQ, which requires you to build a portfolio of on-the-job experience with photographic evidence of you working with all types of installations, containment systems and cable types. All work will need to be signed off by another electrician, and at least 2 pieces of work will need to be reviewed on-site by an assessor for the NVQ to be completed. AM2 AM2 is a 2-day assessment, taken after the completion of the NVQ, with 5 sections. It covers everything from demonstrating a safe isolation procedure, constructing an installation with multiple different cable types, inspecting and testing the installation, fault finding and a multiple choice exam. The installation will include ring and radial cicrcuits, an S-Plan heating circuit, SWA cable termination, RJ45 data cable termination and SY-Flex cable termination. Competent Person This is the golden standard. To be classed as a competent person, you must have gone through level 2, level 3 and your NVQ+AM2. Once you have completed all of this, you are eligable to apply for a gold card, which can cost upward of \u00a3800/year. Workers with a gold card will have work regularly reviewed by their Competent Person Scheme. The gold card gives you the capability to sign off your own work and the work of others, giving you much more freedom in the industry. Qualification Extensions Other notable qualifications include: 2391 2391 is an additional qualification specifically for Inspection and Testing. As part of this qualification, you are required to show competence in inspecting an installation and providing a C-code (C1 - immediate danger, C2 - potential for danger, C3 - improvement recommended, F - Further investigation) based on some images of the installation (not a physical installtion that you can properly inspect..). Following this, you then have a practical test which will involve conducting an Electrical Installation Condition Report (EICR) on 3 circuits, and commissioning a new circuit with an Electrical Installation Certificate (EIC). There will be faults on one or two of the circuits which you will need to identify and C-code, as well as complete both types of certificates and deem whether the installation is satisfactory. There is a written exam which is part of the 2391 which requires a score of 100%. However you do get 2 attempts at the exam and are made aware of the questions which you answered incorrectly in between the attempts, giving you a chance to go and make sure you get them right the second time. The exam looks for very specific terminology in each answer. Part P I have not yet completed my Part P, so I am not able to provide any detail on the requirements.","title":"UK Qualification Requirements"},{"location":"electrical/UK_Qualification_Requirements/#level-2","text":"Level 2 is targetted mostly at domestic installations.","title":"Level 2"},{"location":"electrical/UK_Qualification_Requirements/#practical-exams","text":"There are 2 practical exams at level 2; one for inspection and testing, another for building containment systems. The inspection and testing exam will require you to conduct testing in the correct sequence on a radial, ring and lighting installation; while the containment exam will consist of constructing various containment systems from PVC and metal conduit, to cable tray and metal trunking. All other exams at level 2 are multiple choice, covering topics like health and safety, BS7671, Building Regulations and Electrical Science","title":"Practical Exams"},{"location":"electrical/UK_Qualification_Requirements/#bs7671","text":"One of the hardest exams at level 2 is the BS7671, which is a multiple choice exam which entails navigating the BS7671 regulations to the find answers to the questions. It will require a good amount of confidence in navigating the regulations.","title":"BS7671"},{"location":"electrical/UK_Qualification_Requirements/#building-regulations","text":"The building regulations exam will require you to find information from the Building Regulations for Electricians book. It summarises all the different sections of the building regulations and goes into details about Part P (The section which refers to electrical work). The questions will cover topics such as how much space to leave when drilling into joists and what areas are considered safe zones for electrical cabling to be run without additional protection. Although this covers Part P, completing this exam does not mean that you are Part P qualified.","title":"Building Regulations"},{"location":"electrical/UK_Qualification_Requirements/#electrical-science","text":"Electrical science covers some basic math that applies to electrical work. This includes calculating resistance in series and parallel as well as magnetic flux and power calculations.","title":"Electrical Science"},{"location":"electrical/UK_Qualification_Requirements/#level-3","text":"When training for level 3, I was required to attend a training center for 3 separate weeks, compared to level 2 which can be achieved over multiple weekends.","title":"Level 3"},{"location":"electrical/UK_Qualification_Requirements/#level-3-exams","text":"Level 3 exams include a mix of multiple choice and written exams. One of the written exams include a slightly more challenging electrical science (compared to level 2), this will require a good knowledge of AC motors and the wiring of different motor starter types. It also covers more advanced electrical science principals such as reactance (ICE and ELI). The other exams are around inspection and testing, and environmental technology. On top of the written and multiple choice exams, there are 3 practical exams to complete.","title":"Level 3 Exams"},{"location":"electrical/UK_Qualification_Requirements/#fault-finding","text":"Fault finding is a practical assessment where you are required to identify different fault types and where exactly they are in a circuit. The examiner will report either a piece of equipment not working (open circuit), or a breaker keep on tripping (short circuit). You are required to use your knowledge from inspection and testing to conduct the appropriate test, which should provide the answer of whether it is a short circuit or an open circuit. You will need to specify exactly where the open/short circuit is, and remember that a light bulb can also contribute to the open circuit...","title":"Fault Finding"},{"location":"electrical/UK_Qualification_Requirements/#inspection-and-testing","text":"The inspection and testing is similar to the practical exam of level 2, requiring you to show competence inspecting an installation, safely isolating the circuit, and carrying out a full testing sequence of all the circuits. On completion, you will then need to complete an Electrical Installation Certificate (EIC) with all your test results and state whether the installation is satisfactory based on your results and if the Zs results are satisfactory against the tabulated values in BS7671.","title":"Inspection and Testing"},{"location":"electrical/UK_Qualification_Requirements/#circuit-design","text":"Circuit design is arguably one of the most challenging weeks in level 3. It collates together all the electrical knowledge and electrical science components up until this week. You are provided with a floorplan and a specification document stating the requirements of the installation and given all week (you will need to do some work out of hours) to complete an entire installation plan. Throughout the assignment, the assessor will act as the client of the installation and provide feedback as well as challenge some of the decisions you have made. Firstly, you will need to find the different equipment to go into your installation. Using a site like ScreFix is useful for this, as you can add all the components into your basket as you go, so it is easy to go back and refer to them if needed. The equipment will include multiple 3 phase machines, heating systems, lighting (emergency + non-emergency and needs to be appropriate for floorplan), isolators/switches, socket outlets, circuit breakers, kitchen equipment and anything else appropriate to the specification. Shopping aside, you will need to make note of their power requirements, as you will need this for cable size calculations later. Next you need to create wiring diagrams for all circuits in the installation with correct symbols for equipment and show whether they are single phase or 3 phase circuits, maintained or non-maintained emergency lighting, ring or radial circuits etc.. Using these to-scale diagrams, you can determine the length of wiring needed for the circuit. Using the power ratings of the selected equipment, the length of the circuits and the installation method, you can start calculating the cable size and breaker size requirements referring to BS7671. I highly recommend using a spreadsheet for all this as there are a lot of circuits and a lot of different numbers. You may find you need to change equipment over if it is too power hungry or split the circuits out if all the equipment draws too much power. With all the cable calculations complete and cable sizes determined, you will need to assign the circuits to a 3-phase board location. You will need to apply diversification to your circuits based on information in the On-Site Guide and then assign each circuit to either an L1, L2 or L3 position on the board. You will need to demonstrate that the load is appropriately balanced across the 3 phases.","title":"Circuit Design"},{"location":"electrical/UK_Qualification_Requirements/#nvq","text":"On comlpetion of level 3, you should be ready to begin your NVQ, which requires you to build a portfolio of on-the-job experience with photographic evidence of you working with all types of installations, containment systems and cable types. All work will need to be signed off by another electrician, and at least 2 pieces of work will need to be reviewed on-site by an assessor for the NVQ to be completed.","title":"NVQ"},{"location":"electrical/UK_Qualification_Requirements/#am2","text":"AM2 is a 2-day assessment, taken after the completion of the NVQ, with 5 sections. It covers everything from demonstrating a safe isolation procedure, constructing an installation with multiple different cable types, inspecting and testing the installation, fault finding and a multiple choice exam. The installation will include ring and radial cicrcuits, an S-Plan heating circuit, SWA cable termination, RJ45 data cable termination and SY-Flex cable termination.","title":"AM2"},{"location":"electrical/UK_Qualification_Requirements/#competent-person","text":"This is the golden standard. To be classed as a competent person, you must have gone through level 2, level 3 and your NVQ+AM2. Once you have completed all of this, you are eligable to apply for a gold card, which can cost upward of \u00a3800/year. Workers with a gold card will have work regularly reviewed by their Competent Person Scheme. The gold card gives you the capability to sign off your own work and the work of others, giving you much more freedom in the industry.","title":"Competent Person"},{"location":"electrical/UK_Qualification_Requirements/#qualification-extensions","text":"Other notable qualifications include:","title":"Qualification Extensions"},{"location":"electrical/UK_Qualification_Requirements/#2391","text":"2391 is an additional qualification specifically for Inspection and Testing. As part of this qualification, you are required to show competence in inspecting an installation and providing a C-code (C1 - immediate danger, C2 - potential for danger, C3 - improvement recommended, F - Further investigation) based on some images of the installation (not a physical installtion that you can properly inspect..). Following this, you then have a practical test which will involve conducting an Electrical Installation Condition Report (EICR) on 3 circuits, and commissioning a new circuit with an Electrical Installation Certificate (EIC). There will be faults on one or two of the circuits which you will need to identify and C-code, as well as complete both types of certificates and deem whether the installation is satisfactory. There is a written exam which is part of the 2391 which requires a score of 100%. However you do get 2 attempts at the exam and are made aware of the questions which you answered incorrectly in between the attempts, giving you a chance to go and make sure you get them right the second time. The exam looks for very specific terminology in each answer.","title":"2391"},{"location":"electrical/UK_Qualification_Requirements/#part-p","text":"I have not yet completed my Part P, so I am not able to provide any detail on the requirements.","title":"Part P"},{"location":"electrical/electrical_certificates/","text":"Electrical Installation Certificate An electrical installation certificate should be completed for all work which is not considered to be \"minor works\" (see Minor Works Certificate below). The certificate should be accompanied with a Schedule of Circuit Details and a Schedule of Tests which detail the test results for each of the circuits installed. Minor Works Certificate A minor works certificate is completed whenever the work does not involve an entire new circuit, or the installation is not in a special location such as outside or in a bathroom. Additionally, replacement of a consumer unit would not be covered by minor works. Minor works covers changes such as adding a spur socket off of a ring circuit, or extending a radial lighting circuit with a new light. Electrical Installation Condition Report Electrical condition reports are completed on a periodic basis. The period depends on the type of installation and type of premises. For instance, hospitals require more frequent inspections. The condition report requires a sample of the installation to be inspected and tested rather than the full installation. If there are defects identified during the inspection, fault codes can be assigned based on the severity of the defect. Code Description C1 Immediate danger, replace as a priority C2 Potential to cause harm C3 Recommended Improvement F Further investigation required An example of an F code usage would typically be when there is an area of the site that cannot be accessed during the inspection.","title":"Electrical certificates"},{"location":"electrical/electrical_certificates/#electrical-installation-certificate","text":"An electrical installation certificate should be completed for all work which is not considered to be \"minor works\" (see Minor Works Certificate below). The certificate should be accompanied with a Schedule of Circuit Details and a Schedule of Tests which detail the test results for each of the circuits installed.","title":"Electrical Installation Certificate"},{"location":"electrical/electrical_certificates/#minor-works-certificate","text":"A minor works certificate is completed whenever the work does not involve an entire new circuit, or the installation is not in a special location such as outside or in a bathroom. Additionally, replacement of a consumer unit would not be covered by minor works. Minor works covers changes such as adding a spur socket off of a ring circuit, or extending a radial lighting circuit with a new light.","title":"Minor Works Certificate"},{"location":"electrical/electrical_certificates/#electrical-installation-condition-report","text":"Electrical condition reports are completed on a periodic basis. The period depends on the type of installation and type of premises. For instance, hospitals require more frequent inspections. The condition report requires a sample of the installation to be inspected and tested rather than the full installation. If there are defects identified during the inspection, fault codes can be assigned based on the severity of the defect. Code Description C1 Immediate danger, replace as a priority C2 Potential to cause harm C3 Recommended Improvement F Further investigation required An example of an F code usage would typically be when there is an area of the site that cannot be accessed during the inspection.","title":"Electrical Installation Condition Report"},{"location":"home_lab/Build_you_own_ChatGPT/","text":"ChatGPT On-Premise... but why? ChatGPT has become an incredibly useful tool since its release in 2022. One of the most powerful things you can do with ChatGPT is to prompt it to become your own personal advisor in a wide range of issues. We can leverage it to be our own project manager or even a fitness coach/planner. There is just one issue though; these prompts and the information we provide to ChatGPT can get personal, and all of it goes out to the internet. This data that we provide to OpenAI can be used in model training and has been known to get exposed in the past. Thanksfully, there are hundreds of open source LLMs which we can run ourselves, allowing the information we pass to the model to stay within our home network. With lower-parameter models like llama3:8b and gemma3:12b , it is possible to run an LLM on something like a Mac or a home lab computer with a GPU. They may not be as accurate as what is running behind OpenAI but it can still be a very powerful tool with an increased sense of security. Hardware Recommendations This demo is deploying llama3:8b on an M2 Mac Mini with 16GB of RAM. For context, here is the recommended hardware specs for some of the available opensource LLMs to still achieve a good throughput. Model Min. GPU VRAM llama3:8b 6GB gemma3:12b 9GB qwen2.5:32b 24GB Ollama Ollama is a tool used for running LLMs locally. It can be run very easily with just a docker container: # Make a directory for ollama to download models to mkdir ollama docker run ollama / ollama : 0.10 . 0 - rc3 - p 11434 : 11434 - v ./ ollama : / root /. ollama Or in docker compose: ollama : image : ollama/ollama:0.10.0-rc3 container_name : ollama ports : - 11434:11434 volumes : - ./ollama:/root/.ollama restart : unless-stopped We wont have any models available to talk to yet, but they can be downloaded with the command: docker exec -it ollama ollama pull MODEL_NAME Quantization Since we are running locally, we may not have access to a huge amount of storage, especially if we want to offer a range of models to try within our own ChatGPT. However, we can take advantage of QAT or post-training quantized models, which can reduce their memory footprint significantly. Take Google's gemma3:12b model. The original model takes up 24GB in space, Google leveraged Quantized Aware Training (QAT), which reduces the precision of the models weights and biases from BF16 to Int4, taking the model size down to just 8GB. QAT is supposedly meant to keep the accuracy of the model better and post-training quantization, so we can also try add the gemma3:12b-it-qat model. Open WebUI So we have ollama running, but it is not the most user-friendly interface. Each time we stop ollama, it loses all memory of our conversations. Open WebUI is an open-source tool for providing a ChatGPT-like interface which can be used to point at your own locally-run LLMs. Meaning you can point it at your previously deployed ollama model, completing your completely on-premise ChatGPT. Docker compose example: open-webui: image: ghcr.io/open-webui/open-webui:main container_name: open-webui environment: - OLLAMA_BASE_URL=http://YOUR_IP:11434 - 'WEBUI_SECRET_KEY=' ports: - 3000:8080 volumes: - ./open-webui:/app/backend/data extra_hosts: - host.docker.internal:host-gateway restart: unless-stopped It takes a little while for the container to start, but once it is up, we can hit localhost:3000 and be greated with a very familiar looking interface.","title":"Build you own ChatGPT"},{"location":"home_lab/Build_you_own_ChatGPT/#chatgpt-on-premise-but-why","text":"ChatGPT has become an incredibly useful tool since its release in 2022. One of the most powerful things you can do with ChatGPT is to prompt it to become your own personal advisor in a wide range of issues. We can leverage it to be our own project manager or even a fitness coach/planner. There is just one issue though; these prompts and the information we provide to ChatGPT can get personal, and all of it goes out to the internet. This data that we provide to OpenAI can be used in model training and has been known to get exposed in the past. Thanksfully, there are hundreds of open source LLMs which we can run ourselves, allowing the information we pass to the model to stay within our home network. With lower-parameter models like llama3:8b and gemma3:12b , it is possible to run an LLM on something like a Mac or a home lab computer with a GPU. They may not be as accurate as what is running behind OpenAI but it can still be a very powerful tool with an increased sense of security.","title":"ChatGPT On-Premise... but why?"},{"location":"home_lab/Build_you_own_ChatGPT/#hardware-recommendations","text":"This demo is deploying llama3:8b on an M2 Mac Mini with 16GB of RAM. For context, here is the recommended hardware specs for some of the available opensource LLMs to still achieve a good throughput. Model Min. GPU VRAM llama3:8b 6GB gemma3:12b 9GB qwen2.5:32b 24GB","title":"Hardware Recommendations"},{"location":"home_lab/Build_you_own_ChatGPT/#ollama","text":"Ollama is a tool used for running LLMs locally. It can be run very easily with just a docker container: # Make a directory for ollama to download models to mkdir ollama docker run ollama / ollama : 0.10 . 0 - rc3 - p 11434 : 11434 - v ./ ollama : / root /. ollama Or in docker compose: ollama : image : ollama/ollama:0.10.0-rc3 container_name : ollama ports : - 11434:11434 volumes : - ./ollama:/root/.ollama restart : unless-stopped We wont have any models available to talk to yet, but they can be downloaded with the command: docker exec -it ollama ollama pull MODEL_NAME","title":"Ollama"},{"location":"home_lab/Build_you_own_ChatGPT/#quantization","text":"Since we are running locally, we may not have access to a huge amount of storage, especially if we want to offer a range of models to try within our own ChatGPT. However, we can take advantage of QAT or post-training quantized models, which can reduce their memory footprint significantly. Take Google's gemma3:12b model. The original model takes up 24GB in space, Google leveraged Quantized Aware Training (QAT), which reduces the precision of the models weights and biases from BF16 to Int4, taking the model size down to just 8GB. QAT is supposedly meant to keep the accuracy of the model better and post-training quantization, so we can also try add the gemma3:12b-it-qat model.","title":"Quantization"},{"location":"home_lab/Build_you_own_ChatGPT/#open-webui","text":"So we have ollama running, but it is not the most user-friendly interface. Each time we stop ollama, it loses all memory of our conversations. Open WebUI is an open-source tool for providing a ChatGPT-like interface which can be used to point at your own locally-run LLMs. Meaning you can point it at your previously deployed ollama model, completing your completely on-premise ChatGPT. Docker compose example: open-webui: image: ghcr.io/open-webui/open-webui:main container_name: open-webui environment: - OLLAMA_BASE_URL=http://YOUR_IP:11434 - 'WEBUI_SECRET_KEY=' ports: - 3000:8080 volumes: - ./open-webui:/app/backend/data extra_hosts: - host.docker.internal:host-gateway restart: unless-stopped It takes a little while for the container to start, but once it is up, we can hit localhost:3000 and be greated with a very familiar looking interface.","title":"Open WebUI"},{"location":"networking/FEC/","text":"What is Forward Error Correction? FEC improves bit error rate. Sometimes cyclic redundancy checks or checksums are done to check for errors in transmitted data and determine if it needs to be re-transmitted. FEC removes the need for re-transmitting the data by correcting the data in-place. How does it work? When payloads are transferred between network devices, bits are added to the payload to help reduce errors within the payload. Hamming distance checks the distance between 2 bit streams based on how many bits are incorrect. FEC creates an error correcting code (ECC), which are codes stored in a dictionary on both sides of the connection. The code words are 5 bits (e.g 10101) map to a 2-bit sequence (e.g. 01, 10, 11). Each 2 bits in the transfer data is mapped to a code word, and the final longer string is then what gets transferred between the devices. On receipt of the code words, the receiver checks the code words against the dictionary. If the receivers dictionary contains the code word, then it can be assumed that there were no errors in the transfer. However, if the dictionary does not contain the code word, it will try to identify the most similar code word in the dictionary using the hamming distance. Having multiple bit errors is less likely than a single bit error, so the hamming distance helps determine the most likely intended code word. If the hamming distance determines it could be multiple possible ECCs from the dictionary, the receiver will request that the specific portion of the data is resent. To make a system more robust, code words can be made longer to reduce the likelihood of retransmissions or incorrect decoding. One of the main tradeoffs of FEC is an increased latency due to the increased payload size. There is also additional complexity, since devices on both sides of the connection, including the transceivers, need to support FEC. FEC Options RS (Reed-Solomon) FC (Firecode) Stronger correction but more overhead and latency (Good for DAC cables and long fibre runs) Lighter correction with lower overhead and latency (For 25GB short cable runs) interface Eth X/X fec auto # Negotiate FEC fec rs # Configure Reed-Solomon fec fc # Configure Firecode fec off # No error correction","title":"FEC"},{"location":"networking/FEC/#what-is-forward-error-correction","text":"FEC improves bit error rate. Sometimes cyclic redundancy checks or checksums are done to check for errors in transmitted data and determine if it needs to be re-transmitted. FEC removes the need for re-transmitting the data by correcting the data in-place.","title":"What is Forward Error Correction?"},{"location":"networking/FEC/#how-does-it-work","text":"When payloads are transferred between network devices, bits are added to the payload to help reduce errors within the payload. Hamming distance checks the distance between 2 bit streams based on how many bits are incorrect. FEC creates an error correcting code (ECC), which are codes stored in a dictionary on both sides of the connection. The code words are 5 bits (e.g 10101) map to a 2-bit sequence (e.g. 01, 10, 11). Each 2 bits in the transfer data is mapped to a code word, and the final longer string is then what gets transferred between the devices. On receipt of the code words, the receiver checks the code words against the dictionary. If the receivers dictionary contains the code word, then it can be assumed that there were no errors in the transfer. However, if the dictionary does not contain the code word, it will try to identify the most similar code word in the dictionary using the hamming distance. Having multiple bit errors is less likely than a single bit error, so the hamming distance helps determine the most likely intended code word. If the hamming distance determines it could be multiple possible ECCs from the dictionary, the receiver will request that the specific portion of the data is resent. To make a system more robust, code words can be made longer to reduce the likelihood of retransmissions or incorrect decoding. One of the main tradeoffs of FEC is an increased latency due to the increased payload size. There is also additional complexity, since devices on both sides of the connection, including the transceivers, need to support FEC.","title":"How does it work?"},{"location":"networking/FEC/#fec-options","text":"RS (Reed-Solomon) FC (Firecode) Stronger correction but more overhead and latency (Good for DAC cables and long fibre runs) Lighter correction with lower overhead and latency (For 25GB short cable runs) interface Eth X/X fec auto # Negotiate FEC fec rs # Configure Reed-Solomon fec fc # Configure Firecode fec off # No error correction","title":"FEC Options"},{"location":"networking/IPTables/","text":"What is iptables? IP tables is an interface for managing netfilter, a firewall tool for linux machines It can be used to filter incoming and outgoing packets but also routing puposes. Many Linux distributions are packaged with a default firewall service. For instance, Rocky is packaged with firewalld . It is recommended to remove these installations and start again from scratch when working with iptables . # remove from ubuntu apt purge ufw # remove from rocket dnf remove firewalld How does it work? A table in iptables is a collection of chains. Chains are tags which match packets in their particular state. If a packet matches a particular chain condition, the rules within that chain will determine what happens to the packet. Rules at the top of the chain will take preference. Filter Table Filtering incoming and outgoing traffic. Chains Chain Description Input Chain Packets being received by the host Output Chain Packets being sent by the host Forward Chain For packets where the host is only responsible for forwarding the packet on. NAT Table Redirect connections to other interfaces on the network. Chains Chain Description Output Chain Packets being sent by the host Prerouting Chain Before the route of the packet is determined Postrouting Chain Just before the packet leaves the network interface Mangle Table Modifies aspects of a packet or connection. Chains Chain Description Input Chain Packets being received by the host Output Chain Packets being sent by the host Forward Chain For packets where the host is only responsible for forwarding the packet on. Prerouting Chain Before the route of the packet is determined Postrouting Chain Just before the packet leaves the network interface Targets Targets in iptables determines what will happen to a packet if it matches a particular rule. Targets include: Accept Reject Drop Reject and Drop will both prevent the host from receiving the packet, however the Reject rule will notify the sender, whereas Drop will behave as if the packet was never sent. By default, iptables will accept everything. Commands # List default tables ( Defaults to filter table ) iptables - L # show rules with order iptables - L -- line - numbers # Set default policy for chain iptables -- policy INPUT ACCEPT # Use - A to append to list , - I to insert at top # Drop all incoming packets from 1 . 1 . 1 . 1 iptables - I INPUT - s 1 . 1 . 1 . 1 - j DROP # Block specific protocols / ports iptables - I INPUT - p tcp -- dport 80 - j DROP # Delete rule ( Specify rule number ) iptables - D INPUT 1 # Clear all created rules iptables - f # Save rules to persist / sbin / iptables - save","title":"IPTables"},{"location":"networking/IPTables/#what-is-iptables","text":"IP tables is an interface for managing netfilter, a firewall tool for linux machines It can be used to filter incoming and outgoing packets but also routing puposes. Many Linux distributions are packaged with a default firewall service. For instance, Rocky is packaged with firewalld . It is recommended to remove these installations and start again from scratch when working with iptables . # remove from ubuntu apt purge ufw # remove from rocket dnf remove firewalld","title":"What is iptables?"},{"location":"networking/IPTables/#how-does-it-work","text":"A table in iptables is a collection of chains. Chains are tags which match packets in their particular state. If a packet matches a particular chain condition, the rules within that chain will determine what happens to the packet. Rules at the top of the chain will take preference.","title":"How does it work?"},{"location":"networking/IPTables/#filter-table","text":"Filtering incoming and outgoing traffic. Chains Chain Description Input Chain Packets being received by the host Output Chain Packets being sent by the host Forward Chain For packets where the host is only responsible for forwarding the packet on.","title":"Filter Table"},{"location":"networking/IPTables/#nat-table","text":"Redirect connections to other interfaces on the network. Chains Chain Description Output Chain Packets being sent by the host Prerouting Chain Before the route of the packet is determined Postrouting Chain Just before the packet leaves the network interface","title":"NAT Table"},{"location":"networking/IPTables/#mangle-table","text":"Modifies aspects of a packet or connection. Chains Chain Description Input Chain Packets being received by the host Output Chain Packets being sent by the host Forward Chain For packets where the host is only responsible for forwarding the packet on. Prerouting Chain Before the route of the packet is determined Postrouting Chain Just before the packet leaves the network interface","title":"Mangle Table"},{"location":"networking/IPTables/#targets","text":"Targets in iptables determines what will happen to a packet if it matches a particular rule. Targets include: Accept Reject Drop Reject and Drop will both prevent the host from receiving the packet, however the Reject rule will notify the sender, whereas Drop will behave as if the packet was never sent. By default, iptables will accept everything.","title":"Targets"},{"location":"networking/IPTables/#commands","text":"# List default tables ( Defaults to filter table ) iptables - L # show rules with order iptables - L -- line - numbers # Set default policy for chain iptables -- policy INPUT ACCEPT # Use - A to append to list , - I to insert at top # Drop all incoming packets from 1 . 1 . 1 . 1 iptables - I INPUT - s 1 . 1 . 1 . 1 - j DROP # Block specific protocols / ports iptables - I INPUT - p tcp -- dport 80 - j DROP # Delete rule ( Specify rule number ) iptables - D INPUT 1 # Clear all created rules iptables - f # Save rules to persist / sbin / iptables - save","title":"Commands"},{"location":"networking/MCLAG/","text":"What is MCLAG? Link aggregation provides load balancing across two physical links between 2 devices. Multi-Chassis Link Aggregation Groups allow for a single leaf switch to be connected to two spine switches via a single LAG connection, treating it as a single spine switch to prevent L2 loops, providing redundancy. This eliminates the need for Spanning Tree protocol to block ports and make use of all connections between switches. How to configure MCLAG The commands below are for Dell SONiC switches. Create a LAG Firstly, a LAG needs to be setup on each of the spine switches which will go down to the leaf switch. interface PortChannel X switchport trunk allowed vlan $VLAN_ID Create an MCLAG Domain For each member of the MCLAG, they need to be registered to the MCLAG domain. A switch can only support one MCLAG domain. mclag domain $ID peer-link $INTERFACE The peer link refers to the interface port which links the 2 spine switches together. This can also be configured to be a port channel for extra redundancy. A keepalive link connects MCLAG peer switches to carry out periodic heatbeat messages. This should be configured on each of the peer switches (e.g. spine switches) used. source-ip $PEER1_IP peer-ip $PEER2-IP Finally, the port channel interface needs to be added to the MCLAG interface PortChannel X mclag $ID Gateway Mac MCLAG will use the active switche's system mac address for handling L3 traffic. To have both switches in the pair be able to response on the same mac address, a gateway mac can be configured. # Run on both sides of the mclag mclag gateway-mc $MAC_ADDRESS Running show ip arp will show all IP addresses configured on these switches will resolve to the gateway mac address. If the IP addresses need to explicitly respond from one of the switches in the pair, the SVI can be configured as a mclag-separate-ip interface vlan X mclag-separate-ip MCLAG Commands # See status of mclag show mclag brief # See which southbound interfaces are up / down show mclag interface $ INTERFACE","title":"MCLAG"},{"location":"networking/MCLAG/#what-is-mclag","text":"Link aggregation provides load balancing across two physical links between 2 devices. Multi-Chassis Link Aggregation Groups allow for a single leaf switch to be connected to two spine switches via a single LAG connection, treating it as a single spine switch to prevent L2 loops, providing redundancy. This eliminates the need for Spanning Tree protocol to block ports and make use of all connections between switches.","title":"What is MCLAG?"},{"location":"networking/MCLAG/#how-to-configure-mclag","text":"The commands below are for Dell SONiC switches.","title":"How to configure MCLAG"},{"location":"networking/MCLAG/#create-a-lag","text":"Firstly, a LAG needs to be setup on each of the spine switches which will go down to the leaf switch. interface PortChannel X switchport trunk allowed vlan $VLAN_ID","title":"Create a LAG"},{"location":"networking/MCLAG/#create-an-mclag-domain","text":"For each member of the MCLAG, they need to be registered to the MCLAG domain. A switch can only support one MCLAG domain. mclag domain $ID peer-link $INTERFACE The peer link refers to the interface port which links the 2 spine switches together. This can also be configured to be a port channel for extra redundancy. A keepalive link connects MCLAG peer switches to carry out periodic heatbeat messages. This should be configured on each of the peer switches (e.g. spine switches) used. source-ip $PEER1_IP peer-ip $PEER2-IP Finally, the port channel interface needs to be added to the MCLAG interface PortChannel X mclag $ID","title":"Create an MCLAG Domain"},{"location":"networking/MCLAG/#gateway-mac","text":"MCLAG will use the active switche's system mac address for handling L3 traffic. To have both switches in the pair be able to response on the same mac address, a gateway mac can be configured. # Run on both sides of the mclag mclag gateway-mc $MAC_ADDRESS Running show ip arp will show all IP addresses configured on these switches will resolve to the gateway mac address. If the IP addresses need to explicitly respond from one of the switches in the pair, the SVI can be configured as a mclag-separate-ip interface vlan X mclag-separate-ip","title":"Gateway Mac"},{"location":"networking/MCLAG/#mclag-commands","text":"# See status of mclag show mclag brief # See which southbound interfaces are up / down show mclag interface $ INTERFACE","title":"MCLAG Commands"},{"location":"networking/SONiC_CLI_commands/","text":"# Show VLANs and respective interfaces sudo show vlan brief # Add interface member to vlan (-u for untagged) sudo config vlan member add -u VLAN_ID INTERFACE # Remove interface from VLAN sudo config vlan member del VLAN_ID INTERFACE","title":"SONiC CLI commands"},{"location":"networking/VXLAN/","text":"What is VXLAN? VXLAN uses UDP port 4789 In a physical network (underlay network) setup, the network may be segmented by multiple subnets which require layer 3 routing to route between hosts. This is not ideal in cloud solutions when virtual machines are deployed in the same virtual network but deployed to hosts in separate underlay networks. VXLAN is an overlay network designed so that virtual machines on hosts in two different subnets can talk to each other from what appears to be layer 2. This is achieved with virtual switches running on the hypervisor which encapsulates the layer 2 packets with information that allows it to traverse the layer 3 network between hypervisor hosts. Overlay Networking The only requirement for overlay networking, is that the 2 physical hosts can reach each other on the physical network. Virtual switches need to build tables to associate the virtual machines mac address to a physical network address for the physical server. To start building these tables, ARP messages can be encapsulated into multicast packets to other VSwitch subscribers. New Headers: Layer 3 outer header (dest/from IPs) UDP destination port UDP SRC (Dynaically calaculated) VNI (VXLAN Header - keeps separation of virtual networks) VNIs have 24 bits reserved in the packet header which allows for more than 16M VXLAN identifiers. the Vswitch will strip of the encapsulation on the original packet and forward it to the appropriate VM, meaning the VM will treat it as a standard layer 2 packet.","title":"VXLAN"},{"location":"networking/VXLAN/#what-is-vxlan","text":"VXLAN uses UDP port 4789 In a physical network (underlay network) setup, the network may be segmented by multiple subnets which require layer 3 routing to route between hosts. This is not ideal in cloud solutions when virtual machines are deployed in the same virtual network but deployed to hosts in separate underlay networks. VXLAN is an overlay network designed so that virtual machines on hosts in two different subnets can talk to each other from what appears to be layer 2. This is achieved with virtual switches running on the hypervisor which encapsulates the layer 2 packets with information that allows it to traverse the layer 3 network between hypervisor hosts.","title":"What is VXLAN?"},{"location":"networking/VXLAN/#overlay-networking","text":"The only requirement for overlay networking, is that the 2 physical hosts can reach each other on the physical network. Virtual switches need to build tables to associate the virtual machines mac address to a physical network address for the physical server. To start building these tables, ARP messages can be encapsulated into multicast packets to other VSwitch subscribers. New Headers: Layer 3 outer header (dest/from IPs) UDP destination port UDP SRC (Dynaically calaculated) VNI (VXLAN Header - keeps separation of virtual networks) VNIs have 24 bits reserved in the packet header which allows for more than 16M VXLAN identifiers. the Vswitch will strip of the encapsulation on the original packet and forward it to the appropriate VM, meaning the VM will treat it as a standard layer 2 packet.","title":"Overlay Networking"},{"location":"networking/hybrid_cloud_networking_to_azure/","text":"This doc provides an overview on connecting an on-premise network to Azure cloud via a IPSec VPN Tunnel (not an express-route). Azure Resources The Azure architecture side is built up of 5 main components: Virtual Network, Public IP, Local Network Gateway, Virtual Network Gateway and a Connection Virtual Network The gateway will need to be tied to a virtual network. From this network, it is possible to connect resources or peer other VNets which can communicate with the on-premise network. az network vnet create - g $ RG \\ - n $ VNET_NAME -- address - prefix 10.1.0.0 / 24 \\ -- subnet - name $ SUBNET_NAME \\ -- subnet - prefixes 10.1.0.0 / 24 Take note of the range used here, as this is what will need to be included in the routing table from the on-premise side of the tunnel. Public IP Address The public IP address is required for the on-premise site to connect to. This IP will be assigned to the Virtual Network Gateway az network public-ip create -g $RG -n $IP_NAME Virtual Network Gateway The Virtual Network Gateway has the previously created public IP address assigned to it. Different gateway SKUs will offer different bandwidth az network vnet - gateway create - g $ RG \\ - n vpn - gateway \\ -- public - ip - address $ PUBLIC_IP \\ -- vnet $ VNET_NAME -- gateway - type Vpn \\ -- sku VpnGw1 \\ -- vpn - type RouteBased Local Network Gateway The Local Network Gateway defines the routes for the local network at the other end of the tunnel. It's under this resource that all the IP ranges of the on-premise site are defined. Additionally, we need to set the on-premise public IP for Azure to connect to. In the example below, we are creating a local network gateway for our on-premise network which has a local range of 192.168.1.0/24 and 172.21.21.0/24 az network local - gateway create - g $ RG \\ - n local - gateway \\ -- gateway - ip - address $ ON_PREM_PUBLIC_IP \\ -- local - address - prefixes 192.168.1.0 / 24 172.21.21.0 / 24 Connection The connection resource ties together both the Virtual and Local Network Gateways and specifies the encruyption algorithms to be used. # Create the gateway connection az network vpn-connection create \\ -g $RG -n vpn-connection \\ --vnet-gateway1 $VPN_GATEWAY \\ --local-gateway2 local-gateway \\ --shared-key $RANDOM_GENERATED_SHARED_KEY # Add an IPSec policy defining the encryption algorithnm az network vpn-gateway connection ipsec-policy add \\ --connection-name vpn-connection \\ --dh-group DHGroup14 \\ --ike-encryption AES256 \\ --ike-integrity SHA256 \\ --ipsec-encryption GCMAES256 \\ --ipsec-integrity GCMAES256 \\ --pfs-group ECP256 \\ --resource-group $RG On-Premise From the on-premise firewall, configure an IPSec tunnel with the same encryption rules as specified above and set the same shared key that was specified on the connection resource. This document doesn't specify how to implement any routing protocols to dynamically learn accessible routes in the cloud, so you will need to statically define a route to the cloud based on the virtual network range created earlier with the next hop set to the IPSec tunnel.","title":"Hybrid cloud networking to azure"},{"location":"networking/hybrid_cloud_networking_to_azure/#azure-resources","text":"The Azure architecture side is built up of 5 main components: Virtual Network, Public IP, Local Network Gateway, Virtual Network Gateway and a Connection","title":"Azure Resources"},{"location":"networking/hybrid_cloud_networking_to_azure/#virtual-network","text":"The gateway will need to be tied to a virtual network. From this network, it is possible to connect resources or peer other VNets which can communicate with the on-premise network. az network vnet create - g $ RG \\ - n $ VNET_NAME -- address - prefix 10.1.0.0 / 24 \\ -- subnet - name $ SUBNET_NAME \\ -- subnet - prefixes 10.1.0.0 / 24 Take note of the range used here, as this is what will need to be included in the routing table from the on-premise side of the tunnel.","title":"Virtual Network"},{"location":"networking/hybrid_cloud_networking_to_azure/#public-ip-address","text":"The public IP address is required for the on-premise site to connect to. This IP will be assigned to the Virtual Network Gateway az network public-ip create -g $RG -n $IP_NAME","title":"Public IP Address"},{"location":"networking/hybrid_cloud_networking_to_azure/#virtual-network-gateway","text":"The Virtual Network Gateway has the previously created public IP address assigned to it. Different gateway SKUs will offer different bandwidth az network vnet - gateway create - g $ RG \\ - n vpn - gateway \\ -- public - ip - address $ PUBLIC_IP \\ -- vnet $ VNET_NAME -- gateway - type Vpn \\ -- sku VpnGw1 \\ -- vpn - type RouteBased","title":"Virtual Network Gateway"},{"location":"networking/hybrid_cloud_networking_to_azure/#local-network-gateway","text":"The Local Network Gateway defines the routes for the local network at the other end of the tunnel. It's under this resource that all the IP ranges of the on-premise site are defined. Additionally, we need to set the on-premise public IP for Azure to connect to. In the example below, we are creating a local network gateway for our on-premise network which has a local range of 192.168.1.0/24 and 172.21.21.0/24 az network local - gateway create - g $ RG \\ - n local - gateway \\ -- gateway - ip - address $ ON_PREM_PUBLIC_IP \\ -- local - address - prefixes 192.168.1.0 / 24 172.21.21.0 / 24","title":"Local Network Gateway"},{"location":"networking/hybrid_cloud_networking_to_azure/#connection","text":"The connection resource ties together both the Virtual and Local Network Gateways and specifies the encruyption algorithms to be used. # Create the gateway connection az network vpn-connection create \\ -g $RG -n vpn-connection \\ --vnet-gateway1 $VPN_GATEWAY \\ --local-gateway2 local-gateway \\ --shared-key $RANDOM_GENERATED_SHARED_KEY # Add an IPSec policy defining the encryption algorithnm az network vpn-gateway connection ipsec-policy add \\ --connection-name vpn-connection \\ --dh-group DHGroup14 \\ --ike-encryption AES256 \\ --ike-integrity SHA256 \\ --ipsec-encryption GCMAES256 \\ --ipsec-integrity GCMAES256 \\ --pfs-group ECP256 \\ --resource-group $RG","title":"Connection"},{"location":"networking/hybrid_cloud_networking_to_azure/#on-premise","text":"From the on-premise firewall, configure an IPSec tunnel with the same encryption rules as specified above and set the same shared key that was specified on the connection resource. This document doesn't specify how to implement any routing protocols to dynamically learn accessible routes in the cloud, so you will need to statically define a route to the cloud based on the virtual network range created earlier with the next hop set to the IPSec tunnel.","title":"On-Premise"},{"location":"networking/networking_terminology/","text":"ARP Address Resolution Protocol is an protocol for discovering devices on the local network. A device will send a broadcast message out asking for a device with a particular IP address. This broadcast will reach every device on the layer 2 network. The device that has that IP address will respond to the ARP request with its MAC address. ARP packets are sent when a device does not know how to reach another device on the same network. Once ARP is successful the device will no longer need to send ARP requests everytime it needs to route to that device, it knows the target MAC address and can route the reqeust accordingly. Devices will then build ARP tables, keeping track of the MAC address of particular devices. # Show MAC Address table on mac or linux arp - a Gratuitous ARP Gratuitous ARP works in the opposite direction to ARP. Instead of waiting for a device to ask for an IP address, a device broadcasts its IP address across the network. Broadcast A broadcast message is a network packet send out to every device on the network. When it reaches a network device like a switch, the packet is sent out of every port in the same VLAN. Default Gateway If a device receives a packet with an IP address which is not deemed to be on its local network, then the packet will be sent to the default gateway, which is typically the router on the network. The default gateway will have a routing table on how to route different IP address ranges. Layer 2 Layer 2 refers to networking communication happening within a VLAN where routing is based on MAC addresses. Layer 2 devices build up MAC address tables based on ARP requests. Layer 3 Layer 3 refers to networking communication happening on the IP address level. Network packets will be encapsulated with packet headers specifying a source and destination IP. The destination IP is used to route the packet across the internet. Routers on the internet will build routing tables based on their knowledge of what the next hop should be based on a particular IP range. OSI Model The OSI model refers to the different layers traversed by data between 2 devices. It starts at layer 7, works its way down to the physical networking media between devices. At the end device, it works its way back up to the application layer. Pyhsical Data Link Layer (MAC Address information) Network Layer (IP Address information) Transport Layer (Protocols) Session Layer (Port information) Presentation Layer Application Layer VLAN A VLAN is a segmentation of a layer 2 network. VLANs can be used to reduce broadcast domains and segment the network so only particular devices can communicate with one another. Devices in one VLAN will not receive layer 2 broadcast messages from another VLAN. VXLAN VXLAN is a type of overlay network which allows for 2 virtual machines on difference physical hosts which are in different networks, to communicate over layer 2 as if they are in the same network. VXLAN uses a virtual switch on the physical host to encapsulate the packet with the layer 3 information of the target physical host. The virtual switch needs to build a routing table which maps the MAC addresses of virtual machines to their physical host IP. One way of gatering this information is by being part of a multicast group which allows them to receive ARP packets containing virtual machine information from other hosts.","title":"Networking terminology"},{"location":"networking/networking_terminology/#arp","text":"Address Resolution Protocol is an protocol for discovering devices on the local network. A device will send a broadcast message out asking for a device with a particular IP address. This broadcast will reach every device on the layer 2 network. The device that has that IP address will respond to the ARP request with its MAC address. ARP packets are sent when a device does not know how to reach another device on the same network. Once ARP is successful the device will no longer need to send ARP requests everytime it needs to route to that device, it knows the target MAC address and can route the reqeust accordingly. Devices will then build ARP tables, keeping track of the MAC address of particular devices. # Show MAC Address table on mac or linux arp - a","title":"ARP"},{"location":"networking/networking_terminology/#gratuitous-arp","text":"Gratuitous ARP works in the opposite direction to ARP. Instead of waiting for a device to ask for an IP address, a device broadcasts its IP address across the network.","title":"Gratuitous ARP"},{"location":"networking/networking_terminology/#broadcast","text":"A broadcast message is a network packet send out to every device on the network. When it reaches a network device like a switch, the packet is sent out of every port in the same VLAN.","title":"Broadcast"},{"location":"networking/networking_terminology/#default-gateway","text":"If a device receives a packet with an IP address which is not deemed to be on its local network, then the packet will be sent to the default gateway, which is typically the router on the network. The default gateway will have a routing table on how to route different IP address ranges.","title":"Default Gateway"},{"location":"networking/networking_terminology/#layer-2","text":"Layer 2 refers to networking communication happening within a VLAN where routing is based on MAC addresses. Layer 2 devices build up MAC address tables based on ARP requests.","title":"Layer 2"},{"location":"networking/networking_terminology/#layer-3","text":"Layer 3 refers to networking communication happening on the IP address level. Network packets will be encapsulated with packet headers specifying a source and destination IP. The destination IP is used to route the packet across the internet. Routers on the internet will build routing tables based on their knowledge of what the next hop should be based on a particular IP range.","title":"Layer 3"},{"location":"networking/networking_terminology/#osi-model","text":"The OSI model refers to the different layers traversed by data between 2 devices. It starts at layer 7, works its way down to the physical networking media between devices. At the end device, it works its way back up to the application layer. Pyhsical Data Link Layer (MAC Address information) Network Layer (IP Address information) Transport Layer (Protocols) Session Layer (Port information) Presentation Layer Application Layer","title":"OSI Model"},{"location":"networking/networking_terminology/#vlan","text":"A VLAN is a segmentation of a layer 2 network. VLANs can be used to reduce broadcast domains and segment the network so only particular devices can communicate with one another. Devices in one VLAN will not receive layer 2 broadcast messages from another VLAN.","title":"VLAN"},{"location":"networking/networking_terminology/#vxlan","text":"VXLAN is a type of overlay network which allows for 2 virtual machines on difference physical hosts which are in different networks, to communicate over layer 2 as if they are in the same network. VXLAN uses a virtual switch on the physical host to encapsulate the packet with the layer 3 information of the target physical host. The virtual switch needs to build a routing table which maps the MAC addresses of virtual machines to their physical host IP. One way of gatering this information is by being part of a multicast group which allows them to receive ARP packets containing virtual machine information from other hosts.","title":"VXLAN"},{"location":"networking/radius/","text":"RADIUS Server Quickstart You can quickly get a RADIUS server up and running on Ubuntu with apt install freeradius Users To add a user to the RADIUS server, edit the /etc/freeradius/3.0/users file. Add the lines below above the DEFAULT conditions: user1 Cleartext-Password = \"password\" Clients On the Radius server, you will need to configure a client, which specifies where the connection is coming from and the key they should use to authenticate. The following config should be added to /etc/raddb/clients.conf client CLIENT_NAME { ipaddr = 10.0.0.0 / 24 secret = password } This will allow clients connected in the 10.0.0.0/24 range to authenticate using the key password . With a user and a client in place, we can test the authentication with radtest , which will validate the RADIUS response for a set of given credentials: radtest user1 password 10.0.0.1 10 password Switch Configuration On the Dell switch side, the Radius server host need to be configured to tell the switch how to reach and authenticate with the RADIUS server. By default, RADIUS servers are configured to listen to Auth requests on port 1812 and accounting requests on port 1813 . radius-server host RADIUS_SERVER_IP key 0 \"RADIUS_SECRET\" auth-port 1812 AAA The RADIUS server config can be combined with the aaa configuration on Dell switches to configure how to authenticate and authorize users on the switch. The command supports configuring the switch for a variety of auth methods from RADIUS to TACACS+ or local user databases. aaa authentication login default local group radius The command above configures the switch to first look into the local database for user login, then fallback to the Radius servers defined previously. default refers to the auth list, which covers SSH sessions; alternatively console can be used. The next command determines the authentication preferences for when a user is entering privileged exec mode aaa authentication enable default local group radius To have the switch also call out to RADIUS to enforce authorization, you can configure the following: aaa authorization exec default local group radius This command first checks locally if the user has exec privileges, if not it will go off to RADIUS and check for exec privileges. VTY VTY lines manage the remote access to the switches via SSH or Telnet. For some Dell switches running OS9, additional configuration needs to be added to the VTY lines to ensure that SSH or Telnet connections are authenticated via the RADIUS server. Most modern switches can have up to 15 VTY lines. configure terminal line vty 0 9 login authentication default The config above is saying, to authenticate with VTY lines 0-9, which are used by SSH sessions, refer to the \"default\" auth list.","title":"Radius"},{"location":"networking/radius/#radius-server","text":"","title":"RADIUS Server"},{"location":"networking/radius/#quickstart","text":"You can quickly get a RADIUS server up and running on Ubuntu with apt install freeradius","title":"Quickstart"},{"location":"networking/radius/#users","text":"To add a user to the RADIUS server, edit the /etc/freeradius/3.0/users file. Add the lines below above the DEFAULT conditions: user1 Cleartext-Password = \"password\"","title":"Users"},{"location":"networking/radius/#clients","text":"On the Radius server, you will need to configure a client, which specifies where the connection is coming from and the key they should use to authenticate. The following config should be added to /etc/raddb/clients.conf client CLIENT_NAME { ipaddr = 10.0.0.0 / 24 secret = password } This will allow clients connected in the 10.0.0.0/24 range to authenticate using the key password . With a user and a client in place, we can test the authentication with radtest , which will validate the RADIUS response for a set of given credentials: radtest user1 password 10.0.0.1 10 password","title":"Clients"},{"location":"networking/radius/#switch-configuration","text":"On the Dell switch side, the Radius server host need to be configured to tell the switch how to reach and authenticate with the RADIUS server. By default, RADIUS servers are configured to listen to Auth requests on port 1812 and accounting requests on port 1813 . radius-server host RADIUS_SERVER_IP key 0 \"RADIUS_SECRET\" auth-port 1812","title":"Switch Configuration"},{"location":"networking/radius/#aaa","text":"The RADIUS server config can be combined with the aaa configuration on Dell switches to configure how to authenticate and authorize users on the switch. The command supports configuring the switch for a variety of auth methods from RADIUS to TACACS+ or local user databases. aaa authentication login default local group radius The command above configures the switch to first look into the local database for user login, then fallback to the Radius servers defined previously. default refers to the auth list, which covers SSH sessions; alternatively console can be used. The next command determines the authentication preferences for when a user is entering privileged exec mode aaa authentication enable default local group radius To have the switch also call out to RADIUS to enforce authorization, you can configure the following: aaa authorization exec default local group radius This command first checks locally if the user has exec privileges, if not it will go off to RADIUS and check for exec privileges.","title":"AAA"},{"location":"networking/radius/#vty","text":"VTY lines manage the remote access to the switches via SSH or Telnet. For some Dell switches running OS9, additional configuration needs to be added to the VTY lines to ensure that SSH or Telnet connections are authenticated via the RADIUS server. Most modern switches can have up to 15 VTY lines. configure terminal line vty 0 9 login authentication default The config above is saying, to authenticate with VTY lines 0-9, which are used by SSH sessions, refer to the \"default\" auth list.","title":"VTY"},{"location":"networking/spanning_tree/","text":"Spanning Tree Protocol (STP) STP is a protocol for preventing network loops. It does this by building a topology and blocks ports which are determined to be the slowest or lowest priority path. Switches send out Bridge Protocol Data Units which will contain information about the switch, STP version, health-check intervals, type of BPDU etc. Types of BPDU BPDU Type Description Configuration Determines the Root bridge and builds the STP topology Topology Change Notifies the Root bridge of a change in topology This configuration BPDU information is used to identify the root bridge, which is based on the STP priority set on the switch, as well as the MAC address. This means that switches higher up the stack (core/spine switches), should be configured with a higher priority so none of their ports are blocked. With a root bridge elected, the shortest path to the root is determined based on link speeds. Rapid Spanning Tree (RSTP) RSTP strips out the Topology change BPDUs and replaces them with flags in the configuration BPDUs. Per-VLAN Spanning Tree (PVST) PVST applies the same rules of STP but for each VLAN, so different VLANs can have different root bridges and preferred pathways. BPDUs are first encapsulated with 802.1Q VLAN tags so they are only received by ports listening on the same VLAN. It is recommended to not run this version of STP if there are hundreds or thousands of VLANs on the network, as each VLAN will start a new ST process which can cause a lot of overhead on the network. Multiple Spanning Tree (MST) Using a protocol like PVST can result in a lot of processing going on throughout the network when the network has a lot of VLANs. A protocol like RSTP can be better in that scenario, since there is only one tree to maintain. However, if there is still a requirement to have spanning-tree on a VLAN level, Multiple Spanning Tree (MST) can be used which allows defining spanning tree instances which are assigned for a group of VLANs. This way the network administrators have more control over the number of spanning tree processes happening on the switch.","title":"Spanning tree"},{"location":"networking/spanning_tree/#spanning-tree-protocol-stp","text":"STP is a protocol for preventing network loops. It does this by building a topology and blocks ports which are determined to be the slowest or lowest priority path. Switches send out Bridge Protocol Data Units which will contain information about the switch, STP version, health-check intervals, type of BPDU etc.","title":"Spanning Tree Protocol (STP)"},{"location":"networking/spanning_tree/#types-of-bpdu","text":"BPDU Type Description Configuration Determines the Root bridge and builds the STP topology Topology Change Notifies the Root bridge of a change in topology This configuration BPDU information is used to identify the root bridge, which is based on the STP priority set on the switch, as well as the MAC address. This means that switches higher up the stack (core/spine switches), should be configured with a higher priority so none of their ports are blocked. With a root bridge elected, the shortest path to the root is determined based on link speeds.","title":"Types of BPDU"},{"location":"networking/spanning_tree/#rapid-spanning-tree-rstp","text":"RSTP strips out the Topology change BPDUs and replaces them with flags in the configuration BPDUs.","title":"Rapid Spanning Tree (RSTP)"},{"location":"networking/spanning_tree/#per-vlan-spanning-tree-pvst","text":"PVST applies the same rules of STP but for each VLAN, so different VLANs can have different root bridges and preferred pathways. BPDUs are first encapsulated with 802.1Q VLAN tags so they are only received by ports listening on the same VLAN. It is recommended to not run this version of STP if there are hundreds or thousands of VLANs on the network, as each VLAN will start a new ST process which can cause a lot of overhead on the network.","title":"Per-VLAN Spanning Tree (PVST)"},{"location":"networking/spanning_tree/#multiple-spanning-tree-mst","text":"Using a protocol like PVST can result in a lot of processing going on throughout the network when the network has a lot of VLANs. A protocol like RSTP can be better in that scenario, since there is only one tree to maintain. However, if there is still a requirement to have spanning-tree on a VLAN level, Multiple Spanning Tree (MST) can be used which allows defining spanning tree instances which are assigned for a group of VLANs. This way the network administrators have more control over the number of spanning tree processes happening on the switch.","title":"Multiple Spanning Tree (MST)"},{"location":"openstack/Kayobe_vs_Kolla/","text":"Bot Kayobe and Kolla are openstack tools which leverage Ansible for deploying Openstack components. Kolla Ansible Kolla Ansible is reposnible for deploying Openstack containers on the controller and compute hosts which are part of the overcloud. Command Examples # Bootstrap docker, users and dependencies on target hosts kolla-ansible boostrap-servers # Deploy openstack containers to the host kolla-ansible deploy Kayobe Kayobe is responible for managing all infrastructure responsible in openstack, including the seed host. Where Kolla will just deploy the Openstack containers to overcloud hosts, Kayobe will alos manage the bare metal host, seed node and networking. Command Examples # Install depencies on seed host kayobe seed host configure # Deploy containers used by seed host kayobe seed container deploy # Configure network interfaces etc on overcloud hosts kayobe overcloud host configure # Deploy openstack services on overcloud (calls kolla-ansible under the hood) kayobe overcloud service deploy","title":"Kayobe vs Kolla"},{"location":"openstack/Kayobe_vs_Kolla/#kolla-ansible","text":"Kolla Ansible is reposnible for deploying Openstack containers on the controller and compute hosts which are part of the overcloud.","title":"Kolla Ansible"},{"location":"openstack/Kayobe_vs_Kolla/#command-examples","text":"# Bootstrap docker, users and dependencies on target hosts kolla-ansible boostrap-servers # Deploy openstack containers to the host kolla-ansible deploy","title":"Command Examples"},{"location":"openstack/Kayobe_vs_Kolla/#kayobe","text":"Kayobe is responible for managing all infrastructure responsible in openstack, including the seed host. Where Kolla will just deploy the Openstack containers to overcloud hosts, Kayobe will alos manage the bare metal host, seed node and networking.","title":"Kayobe"},{"location":"openstack/Kayobe_vs_Kolla/#command-examples_1","text":"# Install depencies on seed host kayobe seed host configure # Deploy containers used by seed host kayobe seed container deploy # Configure network interfaces etc on overcloud hosts kayobe overcloud host configure # Deploy openstack services on overcloud (calls kolla-ansible under the hood) kayobe overcloud service deploy","title":"Command Examples"},{"location":"openstack/azimuth/","text":"What is Azimuth? Azimuth leverages infrastructure managed by Openstack to provide self-service applications such as Jupyter notebooks and Kubernetes clusters. Traditional HPC/Big Data approaches were to have individual infrastructure for specific use cases, managed by different tools and teams. Azimuth brings all of these tools together into a single platform to provide use cases for HPC, Big Data and more. How it works Setup Azimuth is setup of a management cluster which runs the Azimuth UI and API responsible for provisioning workload apps and resources, and workload clusters which run in a separate OpenStack project which hosts the self-service clusters and applications. Authentication Azimuth relies on Keycloak as an identity provider, where each Keycloak realm is an Azimuth tenancy. Keycloak is deployed alongside Azimuth within the management cluster. When logging into Azimuth, it is actually authenticating with OpenStack. So the user authenticating with Azimuth will have the same tenancies/projects as the OpenStack user. Networking The workload project requires one external network. If there is more than one, then the Neutron tag is used to select which network to use. Azimuth OpenTofu auto-creates networks within the workload project which defaults to 192.168.3.0/24 . Once provisioned the Ansible will spit out a URL to connect to Azimuth. By default, it will make use of sslip.io domains for automatic DNS handling to be able to route to the Azimuth UI via the nginx ingress controller in the management cluster. Zenith Application Proxy A Zenith client generates a key pair which connects out to a Zenith server. An SSH tunnel is configured between the client and the server for each proxied application. Before this tunnel is established, the client will need to be authenticated through OIDC. A k8s service is generated by the Zenith server which is exposed by the ingress controller. This ensures that IPs are not exposed as the client goes through NAT before it goes out to the Zenith server. The Zenith server will also handle the TLS termination for all proxied services. Deployment Stack Infrastructure Azimuth is deployed using Ansible as an interface, but under the hood it will run OpenTofu to provision the Openstack infrastructure and deploy Kubernetes resources. The OpenTofu state is stored in Kubernetes etcd. Under the hood, when a user creates a tenancy, a namespace is configured on the underlying cluster which will be used to manage the TF state for the platforms that the user will provision. Kubernetes The K8s infrastructure is deployed via ClusterAPI, which allows for the granular behavior of clusters from the node groups to the auto-scaling behavior. Again, the interface for this is via Ansible which templates the Ansible variables into Helm chart values, where Helm charts are used to provision all types of resources from new workload clusters, workstations and applications like Jupyter. Management Cluster Deployment Azimuth deployment configuration is managed through the azimuth-config repository. It requires OS_CLOUD and OS_CLIENT_CONFIG_FILE environment variables to be exported to be able to authenticate to the OpenStack backend. Deployment steps: Upload images to Glance (K8s slurm etc.) Use OpenTofu to deploy a Seed node Seed is configured as a K3s cluster with ClusterAPI deployed (Management Cluster) Deploy a HA K8s cluster thorugh CAPI (K8s API only exposed on the internal network) Octavia Loadbalancers used for Nginx ingress Deploy applications to HA cluster Keycloak Zenith Server CaaS components Azimuth API Harbor (Used as a pull-through cache for images) These above steps deploy the HA cluster which hosts Azimuth and CAPI components which will allow for workload projects and platforms to be provisioned. Platform Deployment When a user selects a new tenancy in the Azimuth UI, the Azimuth API will create Kubernetes CRDs within the cluster which provisions all of the OpenStack infrastructure, sharing the internal network into the OS project and deploying a Zenith client inside the workload project in OpenStack. When a cluster CRD object is created, it triggers a K8s job which executes Ansible to configure the cluster logic in Openstack. These jobs and their progress can be viewed using the K8s API from the HA cluster deployed previously. CaaS Operator Azimuth will create a Lease CRD which creates and deletes Blazar reservations. As a cluster is deployed, it will add a finalizer on the Lease which blocks the lease being deleted until the cluster is ready. When Zenith is deployed, metadata is passed to it as part of the cluster CRD. The cluster node image is configured to look for this metadata. Cluster API Cluster API allows for provisioning of K8s clusters using CRDs. OpenStack is one of many supported infrastructure APIs/clouds which CAPI supports. This allows for completely immutable infrastructure that can be easily destroyed and reproduced. Under the hood of the Azimuth CRDs, Azimuth creates resources to talk to ClusterAPI. The cluster configuration set by the user in the UI is templated into Helm charts which configure the Cluster resources to be deployed onto the management cluster, then telling OpenStack how to provision the infrastructure. Cluster Addons StackHPC provide a cluster addon operator which manages deployments of tools to be deployed ontop of the workload clusters. These tools include tools like the CNI, ingress controller, device drivers and monitoring. The charts which deploy resources for this operator can be found here One of the useful tools deployed, is the node-feature-discovery which will identify PCIe devices on the host and appropriately label the host with its features. This provides the information for addons for device drivers to be added to the nodes for things like GPU accessibility. Addons are deployed using Helm charts where each chart deployment is its own CRD/resource on the management cluster which has the context of the remote workload cluster for the chart resources to be installed on to. Cluster Templates Cluster Templates are another StackHPC CRD which defines a flavour of cluster such as the available K8s versions and the CNI types available. App Templates and Applications App templates pre-define which applications are available in a platform/project. StackHPC provide helm charts to deploy particular applications from Azimuth. Under the hood, the Azimuth API is installing these application charts, using the HelmRelase StackHPC CRD, onto the workload cluster to deploy the application resources. Azimuth Config Repo The Azimuth configuration repo allows you to define different Azimuth deployment environments. It comes with pre-defined Ansible configurations for deploying HA environments just by pointing an environment's ansible.cfg inventory at ../ha/inventory .","title":"Azimuth"},{"location":"openstack/azimuth/#what-is-azimuth","text":"Azimuth leverages infrastructure managed by Openstack to provide self-service applications such as Jupyter notebooks and Kubernetes clusters. Traditional HPC/Big Data approaches were to have individual infrastructure for specific use cases, managed by different tools and teams. Azimuth brings all of these tools together into a single platform to provide use cases for HPC, Big Data and more.","title":"What is Azimuth?"},{"location":"openstack/azimuth/#how-it-works","text":"","title":"How it works"},{"location":"openstack/azimuth/#setup","text":"Azimuth is setup of a management cluster which runs the Azimuth UI and API responsible for provisioning workload apps and resources, and workload clusters which run in a separate OpenStack project which hosts the self-service clusters and applications.","title":"Setup"},{"location":"openstack/azimuth/#authentication","text":"Azimuth relies on Keycloak as an identity provider, where each Keycloak realm is an Azimuth tenancy. Keycloak is deployed alongside Azimuth within the management cluster. When logging into Azimuth, it is actually authenticating with OpenStack. So the user authenticating with Azimuth will have the same tenancies/projects as the OpenStack user.","title":"Authentication"},{"location":"openstack/azimuth/#networking","text":"The workload project requires one external network. If there is more than one, then the Neutron tag is used to select which network to use. Azimuth OpenTofu auto-creates networks within the workload project which defaults to 192.168.3.0/24 . Once provisioned the Ansible will spit out a URL to connect to Azimuth. By default, it will make use of sslip.io domains for automatic DNS handling to be able to route to the Azimuth UI via the nginx ingress controller in the management cluster.","title":"Networking"},{"location":"openstack/azimuth/#zenith-application-proxy","text":"A Zenith client generates a key pair which connects out to a Zenith server. An SSH tunnel is configured between the client and the server for each proxied application. Before this tunnel is established, the client will need to be authenticated through OIDC. A k8s service is generated by the Zenith server which is exposed by the ingress controller. This ensures that IPs are not exposed as the client goes through NAT before it goes out to the Zenith server. The Zenith server will also handle the TLS termination for all proxied services.","title":"Zenith Application Proxy"},{"location":"openstack/azimuth/#deployment-stack","text":"Infrastructure Azimuth is deployed using Ansible as an interface, but under the hood it will run OpenTofu to provision the Openstack infrastructure and deploy Kubernetes resources. The OpenTofu state is stored in Kubernetes etcd. Under the hood, when a user creates a tenancy, a namespace is configured on the underlying cluster which will be used to manage the TF state for the platforms that the user will provision. Kubernetes The K8s infrastructure is deployed via ClusterAPI, which allows for the granular behavior of clusters from the node groups to the auto-scaling behavior. Again, the interface for this is via Ansible which templates the Ansible variables into Helm chart values, where Helm charts are used to provision all types of resources from new workload clusters, workstations and applications like Jupyter.","title":"Deployment Stack"},{"location":"openstack/azimuth/#management-cluster-deployment","text":"Azimuth deployment configuration is managed through the azimuth-config repository. It requires OS_CLOUD and OS_CLIENT_CONFIG_FILE environment variables to be exported to be able to authenticate to the OpenStack backend. Deployment steps: Upload images to Glance (K8s slurm etc.) Use OpenTofu to deploy a Seed node Seed is configured as a K3s cluster with ClusterAPI deployed (Management Cluster) Deploy a HA K8s cluster thorugh CAPI (K8s API only exposed on the internal network) Octavia Loadbalancers used for Nginx ingress Deploy applications to HA cluster Keycloak Zenith Server CaaS components Azimuth API Harbor (Used as a pull-through cache for images) These above steps deploy the HA cluster which hosts Azimuth and CAPI components which will allow for workload projects and platforms to be provisioned.","title":"Management Cluster Deployment"},{"location":"openstack/azimuth/#platform-deployment","text":"When a user selects a new tenancy in the Azimuth UI, the Azimuth API will create Kubernetes CRDs within the cluster which provisions all of the OpenStack infrastructure, sharing the internal network into the OS project and deploying a Zenith client inside the workload project in OpenStack. When a cluster CRD object is created, it triggers a K8s job which executes Ansible to configure the cluster logic in Openstack. These jobs and their progress can be viewed using the K8s API from the HA cluster deployed previously. CaaS Operator Azimuth will create a Lease CRD which creates and deletes Blazar reservations. As a cluster is deployed, it will add a finalizer on the Lease which blocks the lease being deleted until the cluster is ready. When Zenith is deployed, metadata is passed to it as part of the cluster CRD. The cluster node image is configured to look for this metadata. Cluster API Cluster API allows for provisioning of K8s clusters using CRDs. OpenStack is one of many supported infrastructure APIs/clouds which CAPI supports. This allows for completely immutable infrastructure that can be easily destroyed and reproduced. Under the hood of the Azimuth CRDs, Azimuth creates resources to talk to ClusterAPI. The cluster configuration set by the user in the UI is templated into Helm charts which configure the Cluster resources to be deployed onto the management cluster, then telling OpenStack how to provision the infrastructure. Cluster Addons StackHPC provide a cluster addon operator which manages deployments of tools to be deployed ontop of the workload clusters. These tools include tools like the CNI, ingress controller, device drivers and monitoring. The charts which deploy resources for this operator can be found here One of the useful tools deployed, is the node-feature-discovery which will identify PCIe devices on the host and appropriately label the host with its features. This provides the information for addons for device drivers to be added to the nodes for things like GPU accessibility. Addons are deployed using Helm charts where each chart deployment is its own CRD/resource on the management cluster which has the context of the remote workload cluster for the chart resources to be installed on to. Cluster Templates Cluster Templates are another StackHPC CRD which defines a flavour of cluster such as the available K8s versions and the CNI types available. App Templates and Applications App templates pre-define which applications are available in a platform/project. StackHPC provide helm charts to deploy particular applications from Azimuth. Under the hood, the Azimuth API is installing these application charts, using the HelmRelase StackHPC CRD, onto the workload cluster to deploy the application resources.","title":"Platform Deployment"},{"location":"openstack/azimuth/#azimuth-config-repo","text":"The Azimuth configuration repo allows you to define different Azimuth deployment environments. It comes with pre-defined Ansible configurations for deploying HA environments just by pointing an environment's ansible.cfg inventory at ../ha/inventory .","title":"Azimuth Config Repo"},{"location":"openstack/openstack_baremetal_enroll/","text":"Enroll Process Attempts PXE boot responded to by DHCP server running on controller nodes PXE deploys the IPA ramdisk image This talks to the Ironic API to determine the next state the node needs to run Ironic Python Agent (IPA) As the baremetal machine transitions through the different states, the host is booted with an Ironic python agent deployed as a ramdisk on the server. This agent helps control how images are deployed to the server. The ironic controller will make API calls to the agent to help the machine through the provisioning lifecycle. It is possible to debug issues with the enroll stages by configuring a user on the IPA ramdisk image which can be SSH'd into during the boot process. States When a new baremetal node is ingested into Ironic, it goes through a couple of states before it can be available for scheduling. Enroll This is the very first stage of a baremetal host. It cannot yet be used for contacting the BMC or provisioning. From this state, the node must be explicitly promoted to a \"manageable\" state in order to start running inspections etc. Manageable A baremetal node can be taken from the enroll state, to the manage state using openstack baremetal node manage . This state ensures that Ironic can talk to the node's BMC, and allows for administrative tasks such as cleaning, inspection and updating hardware properties. Inspecting There are 2 types of inspetion done by Ironic: In-Band This is run on the new baremetal node itself. It PXE boots a RAMDisk image which collects information about the host such as CPU cores, RAM, Disk storage etc. While this approach is slower than out-of-band, it is able to gather much more accurate information about the host, and support a wider range of hardware information. Out-of-Band This runs from the Ironic Conductor service and collects hardware information about the host remotely via the BMC using IPMI or Redfish. Cleaning Ironic will put a node into a state of cleaning before provisioning or after deletion. This is to ensure that it is fully clean before anything is run on the host. This will include wiping the disk so there is no left over state. Available Before a node can be deployed, the baremetal host must first be placed in an \"Available\" state. It is not possible to deploy a node from the \"manageable\" state. Baremetal nodes can be made available with openstack baremetal node provide","title":"Openstack baremetal enroll"},{"location":"openstack/openstack_baremetal_enroll/#enroll-process","text":"Attempts PXE boot responded to by DHCP server running on controller nodes PXE deploys the IPA ramdisk image This talks to the Ironic API to determine the next state the node needs to run","title":"Enroll Process"},{"location":"openstack/openstack_baremetal_enroll/#ironic-python-agent-ipa","text":"As the baremetal machine transitions through the different states, the host is booted with an Ironic python agent deployed as a ramdisk on the server. This agent helps control how images are deployed to the server. The ironic controller will make API calls to the agent to help the machine through the provisioning lifecycle. It is possible to debug issues with the enroll stages by configuring a user on the IPA ramdisk image which can be SSH'd into during the boot process.","title":"Ironic Python Agent (IPA)"},{"location":"openstack/openstack_baremetal_enroll/#states","text":"When a new baremetal node is ingested into Ironic, it goes through a couple of states before it can be available for scheduling.","title":"States"},{"location":"openstack/openstack_baremetal_enroll/#enroll","text":"This is the very first stage of a baremetal host. It cannot yet be used for contacting the BMC or provisioning. From this state, the node must be explicitly promoted to a \"manageable\" state in order to start running inspections etc.","title":"Enroll"},{"location":"openstack/openstack_baremetal_enroll/#manageable","text":"A baremetal node can be taken from the enroll state, to the manage state using openstack baremetal node manage . This state ensures that Ironic can talk to the node's BMC, and allows for administrative tasks such as cleaning, inspection and updating hardware properties.","title":"Manageable"},{"location":"openstack/openstack_baremetal_enroll/#inspecting","text":"There are 2 types of inspetion done by Ironic: In-Band This is run on the new baremetal node itself. It PXE boots a RAMDisk image which collects information about the host such as CPU cores, RAM, Disk storage etc. While this approach is slower than out-of-band, it is able to gather much more accurate information about the host, and support a wider range of hardware information. Out-of-Band This runs from the Ironic Conductor service and collects hardware information about the host remotely via the BMC using IPMI or Redfish.","title":"Inspecting"},{"location":"openstack/openstack_baremetal_enroll/#cleaning","text":"Ironic will put a node into a state of cleaning before provisioning or after deletion. This is to ensure that it is fully clean before anything is run on the host. This will include wiping the disk so there is no left over state.","title":"Cleaning"},{"location":"openstack/openstack_baremetal_enroll/#available","text":"Before a node can be deployed, the baremetal host must first be placed in an \"Available\" state. It is not possible to deploy a node from the \"manageable\" state. Baremetal nodes can be made available with openstack baremetal node provide","title":"Available"},{"location":"openstack/openstack_host_aggregates/","text":"What are host aggregates? Openstack hosts can be assigned to aggregates which identify hosts using a key-value pair. It is a mechanism for partitioning hosts and restricting which tenants have access to them. Hosts can be assigned to multiple aggregates which different ways of restricting access based on the properties assigned to the aggregate. Compute Filters defines the different types of filters that can be placed to restrict access to specific hosts. Multi-Tenancy Isolation Aggregates can be assigned a filter_tenant_id metadata key which restricts which openstack projects have access to the hosts in that aggregate. This requires AggregateMultiTenancyIsolation to be listed as an enabled filter in nova.conf . # Filter aggregate to only permit certain projects access to hosts openstack aggregate set compute - agg -- property filter_tenant_id = $PROJECT_ID1 , $PROJECT_ID2 If a project does not exist in the filter_tenant on the aggregate, it can only use hosts that are assigned to other aggregates without this filter. Since hosts can belong in multiple aggregates, Nova uses a logical OR to determine if the host is schedulable. So if a host belongs to another aggregate which permits a project or is simply open to all projects, then the host can still be used despite another aggregate not permitting it. Commands # Create an aggregate openstack aggregate create -- zone nova agg1 # Set a property on the aggregate openstack aggregate set agg1 -- property gpu = 2 # Add a host to the aggregate openstack aggregate add host agg1 $HOST_ID","title":"Openstack host aggregates"},{"location":"openstack/openstack_host_aggregates/#what-are-host-aggregates","text":"Openstack hosts can be assigned to aggregates which identify hosts using a key-value pair. It is a mechanism for partitioning hosts and restricting which tenants have access to them. Hosts can be assigned to multiple aggregates which different ways of restricting access based on the properties assigned to the aggregate. Compute Filters defines the different types of filters that can be placed to restrict access to specific hosts.","title":"What are host aggregates?"},{"location":"openstack/openstack_host_aggregates/#multi-tenancy-isolation","text":"Aggregates can be assigned a filter_tenant_id metadata key which restricts which openstack projects have access to the hosts in that aggregate. This requires AggregateMultiTenancyIsolation to be listed as an enabled filter in nova.conf . # Filter aggregate to only permit certain projects access to hosts openstack aggregate set compute - agg -- property filter_tenant_id = $PROJECT_ID1 , $PROJECT_ID2 If a project does not exist in the filter_tenant on the aggregate, it can only use hosts that are assigned to other aggregates without this filter. Since hosts can belong in multiple aggregates, Nova uses a logical OR to determine if the host is schedulable. So if a host belongs to another aggregate which permits a project or is simply open to all projects, then the host can still be used despite another aggregate not permitting it.","title":"Multi-Tenancy Isolation"},{"location":"openstack/openstack_host_aggregates/#commands","text":"# Create an aggregate openstack aggregate create -- zone nova agg1 # Set a property on the aggregate openstack aggregate set agg1 -- property gpu = 2 # Add a host to the aggregate openstack aggregate add host agg1 $HOST_ID","title":"Commands"},{"location":"openstack/openstack_images/","text":"Baremetal Images and User Images Baremetal servers managed in Ironic require their own deploy images which prepare the BM server for OS deployments. On top of these baremetal images, user images are deployed which are the images actually used by the end user. Disk Image Builder You can install the diskimage-builder package to be able to build user images.","title":"Openstack images"},{"location":"openstack/openstack_images/#baremetal-images-and-user-images","text":"Baremetal servers managed in Ironic require their own deploy images which prepare the BM server for OS deployments. On top of these baremetal images, user images are deployed which are the images actually used by the end user.","title":"Baremetal Images and User Images"},{"location":"openstack/openstack_images/#disk-image-builder","text":"You can install the diskimage-builder package to be able to build user images.","title":"Disk Image Builder"},{"location":"openstack/openstack_kolla_deployment/","text":"Openstack can be deployed in a number of ways. A popular tool for deployment is Kayobe which deploys the Openstack services to machines as docker containers. Kayobe uses kolla-ansible to automate the deployment of Openstack services. Kayobe # Configure Seed hypervisor kayobe seed hypervisor host configure # Configure a Seed host kayobe seed host configure # Configure an overcloud host kayobe overcloud host configure Kayobe has global configuration for each of these type of hosts under $KAYOBE_CONFIG_PATH. The configuration files names represent the host type. seed.yml controller.yml compute.yml Service Deployment With the Kayobe host configured, the services can be deployed. # Deploy services for seed host kayobe seed service deploy # Deploy services for overcloud hosts kayobe overcloud service deploy Service Upgrade This docs assumes an Openstack deployment with Kayobe. Official Github docs for updates can be found here Overcloud # Make database backup kayobe overcloud database backup # Pull host containers kayobe overcloud container image pull # Update host packages kayobe overcloud host package update -- packages \"*\" # Update host components (pip packages etc.) kayobe overcloud host upgrade # Stop health manager service (from controllers) # Checking for loadbalancer states to see if they need to failover (upgrade can cause them to go into an error state) # Loadbalancers wont come out of error state docker stop octavia_health_manager # Upgrade services # Skip sensitive services like nova and networking services kayobe overcloud service upgrade -- kolla - skip - tags neutron , nova , openvswitch","title":"Openstack kolla deployment"},{"location":"openstack/openstack_kolla_deployment/#kayobe","text":"# Configure Seed hypervisor kayobe seed hypervisor host configure # Configure a Seed host kayobe seed host configure # Configure an overcloud host kayobe overcloud host configure Kayobe has global configuration for each of these type of hosts under $KAYOBE_CONFIG_PATH. The configuration files names represent the host type. seed.yml controller.yml compute.yml","title":"Kayobe"},{"location":"openstack/openstack_kolla_deployment/#service-deployment","text":"With the Kayobe host configured, the services can be deployed. # Deploy services for seed host kayobe seed service deploy # Deploy services for overcloud hosts kayobe overcloud service deploy","title":"Service Deployment"},{"location":"openstack/openstack_kolla_deployment/#service-upgrade","text":"This docs assumes an Openstack deployment with Kayobe. Official Github docs for updates can be found here","title":"Service Upgrade"},{"location":"openstack/openstack_kolla_deployment/#overcloud","text":"# Make database backup kayobe overcloud database backup # Pull host containers kayobe overcloud container image pull # Update host packages kayobe overcloud host package update -- packages \"*\" # Update host components (pip packages etc.) kayobe overcloud host upgrade # Stop health manager service (from controllers) # Checking for loadbalancer states to see if they need to failover (upgrade can cause them to go into an error state) # Loadbalancers wont come out of error state docker stop octavia_health_manager # Upgrade services # Skip sensitive services like nova and networking services kayobe overcloud service upgrade -- kolla - skip - tags neutron , nova , openvswitch","title":"Overcloud"},{"location":"openstack/openstack_networking/","text":"Networking Options Openstack provides 2 options for networking. Provider Networks Openstack bridges virtual networks to physical networks and relies on physical network infrastructure for layer 3 routing. A DHCP service provides IP address information to instances. This option lacks support for self-service networks as well as LBaaS and FWaaS Self-service Networks Self-service networks use segmentation methods such as VXLAN and routes virtual networks to physical networks using NAT. Using the benefits and dynamic approach of VXLAN, users can create their own virtual networks without needing to rely on changes to the underlying physical network. Network Architectures Open VSwitch (OVS) Open VSwitch acts like a traffic manager. Neutron feeds OVS with traffic routing rules which define which packets should be sent where. The main drawback of OVS is that each time a VM moves or changes are made to the system, Neutron needs to notify all the OVS agents in the system with the updated routing rules. When deploying Openstack with Kolla, each host will get an openvswitch-vswitchd container deployed. This is responsible for running OpenVSwitch and handling bridge networking across the virtual machines deployed on the host. Open Virtual Network (OVN) Instead of individual agents, there is a centralised controller with Northbound and Southbound databases. Every node will still be running Open VSwitch, but each OVS agent will download the data from the OVN controller. Now, whenever there is a change to the network, the network rules only need to be updated in one place, making the setup much more scalable. Network Automation during Enroll As a baremetal host transitions through different states such as cleaning, inspection and provisioning, openstack can be configured to automatically configure the leaf switches to update switch ports to the appropriate VLAN required for PXE booting. This is handled using Netmiko, a Python-based tool used for automating the configuration of switches. In particular, openstack network-generic-switch which is a wrapping layer between Openstack and Netmiko. Network Types When deploying with Kayobe, there are a few classes of networks which are defined by default: Overcloud Out-of-band | Used by seed host to access the OOB mgmt controllers of the bare metal hosts Overcloud Provisioning | Used by the seed host to provision cloud hosts Workload Out-of-band | Used by overcloud hsots to access the PPB mgmt controllers of bare metal hosts Workload Provisioning | Used by the cloud hosts to provision the baremetal compute hosts Internal | For internal and admin API endpoints Public | Public API endpoints External | Provide external network access for hosts in the system","title":"Openstack networking"},{"location":"openstack/openstack_networking/#networking-options","text":"Openstack provides 2 options for networking.","title":"Networking Options"},{"location":"openstack/openstack_networking/#provider-networks","text":"Openstack bridges virtual networks to physical networks and relies on physical network infrastructure for layer 3 routing. A DHCP service provides IP address information to instances. This option lacks support for self-service networks as well as LBaaS and FWaaS","title":"Provider Networks"},{"location":"openstack/openstack_networking/#self-service-networks","text":"Self-service networks use segmentation methods such as VXLAN and routes virtual networks to physical networks using NAT. Using the benefits and dynamic approach of VXLAN, users can create their own virtual networks without needing to rely on changes to the underlying physical network.","title":"Self-service Networks"},{"location":"openstack/openstack_networking/#network-architectures","text":"","title":"Network Architectures"},{"location":"openstack/openstack_networking/#open-vswitch-ovs","text":"Open VSwitch acts like a traffic manager. Neutron feeds OVS with traffic routing rules which define which packets should be sent where. The main drawback of OVS is that each time a VM moves or changes are made to the system, Neutron needs to notify all the OVS agents in the system with the updated routing rules. When deploying Openstack with Kolla, each host will get an openvswitch-vswitchd container deployed. This is responsible for running OpenVSwitch and handling bridge networking across the virtual machines deployed on the host.","title":"Open VSwitch (OVS)"},{"location":"openstack/openstack_networking/#open-virtual-network-ovn","text":"Instead of individual agents, there is a centralised controller with Northbound and Southbound databases. Every node will still be running Open VSwitch, but each OVS agent will download the data from the OVN controller. Now, whenever there is a change to the network, the network rules only need to be updated in one place, making the setup much more scalable.","title":"Open Virtual Network (OVN)"},{"location":"openstack/openstack_networking/#network-automation-during-enroll","text":"As a baremetal host transitions through different states such as cleaning, inspection and provisioning, openstack can be configured to automatically configure the leaf switches to update switch ports to the appropriate VLAN required for PXE booting. This is handled using Netmiko, a Python-based tool used for automating the configuration of switches. In particular, openstack network-generic-switch which is a wrapping layer between Openstack and Netmiko.","title":"Network Automation during Enroll"},{"location":"openstack/openstack_networking/#network-types","text":"When deploying with Kayobe, there are a few classes of networks which are defined by default: Overcloud Out-of-band | Used by seed host to access the OOB mgmt controllers of the bare metal hosts Overcloud Provisioning | Used by the seed host to provision cloud hosts Workload Out-of-band | Used by overcloud hsots to access the PPB mgmt controllers of bare metal hosts Workload Provisioning | Used by the cloud hosts to provision the baremetal compute hosts Internal | For internal and admin API endpoints Public | Public API endpoints External | Provide external network access for hosts in the system","title":"Network Types"},{"location":"openstack/openstack_overview/","text":"OpenStack Hardware Openstack is built up of multiple hareware components. The core components are the controller and compute nodes. Optional additional hardware include a block and object storage node. Seed Host The Seed host manages all the provisioning of the controller, compute, networking and storage hosts. The seed host runs a bifros container which is used to provision the cloud hosts (controllers + compute etc.). Controller The controller is responsible for running identity services, image services, placement services and management portions of compute and networking agents. It can optionally run portions of the block storage, objects storage and orchestration. Networking Network hosts run the neutron networking services and load balancers for API services. Compute Compute hosts can be either hypervisors or baremetal hosts. Baremetal hosts will be managed by the Ironic service. Alternatively, hypervisor hosts will run additional compute, networking and storage services for managing Virtual Machines. Block storage This node contains the disks Traffic between this node and compute nodes uses the management network. It is recommended for production setups to have a dedicated storage network to increase performance and security. OpenStack services Nova Nova is the service for handling Virtual machines on hosts. Under the hood it uses KVM ( virsh ), a linux hypervisor built into a kernel module. So from a host running Nova, you can run virsh list to view the active VMs on that host. Ironic Ironic is the service which manages the baremetal hosts within Openstack. New baremetal hosts are registered with the Ironic service. Ironic keeps track of the state of the host. See this diagram for more details: https://docs.openstack.org/ironic/latest/_images/states.svg. Detail of the states is also provided in Openstack Ironic States You can manage baremetal nodes in Ironic with the openstack baremetal node CLI, which allows for a host to be transitioned through different states and be put into maintenance mode. Once a host is managed by Ironic, it will be viewable by the openstack admin using the command below: openstack baremetal node list Neutron Neutron is the networking service which handles the configuration of network interfaces on the hosts and supporting tools like VXLAN. The Neutron server routes API requests to the appropriate network plugins based on the request. More detail can be found in Openstack Networking . Storage Service Description Cinder Block storage service responsible for integrating with block storage devices to provision volumes and mount volumes to hosts. Swift Object storage for storing VM images and data. Manila Provides file shares to virtual machines Glance Manages images in Openstack which are used for VM provisioning. Octavia Manages Loadbalancer resources.","title":"Openstack overview"},{"location":"openstack/openstack_overview/#openstack-hardware","text":"Openstack is built up of multiple hareware components. The core components are the controller and compute nodes. Optional additional hardware include a block and object storage node.","title":"OpenStack Hardware"},{"location":"openstack/openstack_overview/#seed-host","text":"The Seed host manages all the provisioning of the controller, compute, networking and storage hosts. The seed host runs a bifros container which is used to provision the cloud hosts (controllers + compute etc.).","title":"Seed Host"},{"location":"openstack/openstack_overview/#controller","text":"The controller is responsible for running identity services, image services, placement services and management portions of compute and networking agents. It can optionally run portions of the block storage, objects storage and orchestration.","title":"Controller"},{"location":"openstack/openstack_overview/#networking","text":"Network hosts run the neutron networking services and load balancers for API services.","title":"Networking"},{"location":"openstack/openstack_overview/#compute","text":"Compute hosts can be either hypervisors or baremetal hosts. Baremetal hosts will be managed by the Ironic service. Alternatively, hypervisor hosts will run additional compute, networking and storage services for managing Virtual Machines.","title":"Compute"},{"location":"openstack/openstack_overview/#block-storage","text":"This node contains the disks Traffic between this node and compute nodes uses the management network. It is recommended for production setups to have a dedicated storage network to increase performance and security.","title":"Block storage"},{"location":"openstack/openstack_overview/#openstack-services","text":"","title":"OpenStack services"},{"location":"openstack/openstack_overview/#nova","text":"Nova is the service for handling Virtual machines on hosts. Under the hood it uses KVM ( virsh ), a linux hypervisor built into a kernel module. So from a host running Nova, you can run virsh list to view the active VMs on that host.","title":"Nova"},{"location":"openstack/openstack_overview/#ironic","text":"Ironic is the service which manages the baremetal hosts within Openstack. New baremetal hosts are registered with the Ironic service. Ironic keeps track of the state of the host. See this diagram for more details: https://docs.openstack.org/ironic/latest/_images/states.svg. Detail of the states is also provided in Openstack Ironic States You can manage baremetal nodes in Ironic with the openstack baremetal node CLI, which allows for a host to be transitioned through different states and be put into maintenance mode. Once a host is managed by Ironic, it will be viewable by the openstack admin using the command below: openstack baremetal node list","title":"Ironic"},{"location":"openstack/openstack_overview/#neutron","text":"Neutron is the networking service which handles the configuration of network interfaces on the hosts and supporting tools like VXLAN. The Neutron server routes API requests to the appropriate network plugins based on the request. More detail can be found in Openstack Networking .","title":"Neutron"},{"location":"openstack/openstack_overview/#storage","text":"Service Description Cinder Block storage service responsible for integrating with block storage devices to provision volumes and mount volumes to hosts. Swift Object storage for storing VM images and data. Manila Provides file shares to virtual machines","title":"Storage"},{"location":"openstack/openstack_overview/#glance","text":"Manages images in Openstack which are used for VM provisioning.","title":"Glance"},{"location":"openstack/openstack_overview/#octavia","text":"Manages Loadbalancer resources.","title":"Octavia"},{"location":"storage/RAID/","text":"RAID Types RAID 0 (Striping) Stripes data across multiple disks to improve performance. Provides faster read/write speeds but offers no resiliency meaning it cannot tolerate any drive failures. RAID 1 (Mirroring) Requires a minimum of 2 drives. Creates copies of the data across the drives so the capacity of the storage is n/2 . Can withstand a single drive failure. RAID 5 (Striping with Distributed Parity) Requires a minimum of 3 drives to distribute data and parity information across. Can only withstand a single drive failure. RAID 6 (Striping with Double Parity) Distributes data across multiple drives but uses independent sets of parity data. This allows for it to withstand 2 drive failures at the same time. Requires a minimum of 4 drives. RAID 10 (Striping + Mirroring) Requires a minimum of 4 drives to create independent mirrored sets where data is striped across drives within the set. This provides the high performance of RAID 0 (still slower than RAID 5 or 6), and the fault tolerance provided by RAID 1. RAID Policies Read Ahead Read sequential data ahead of time and store it in the cache to make read operations faster. Altenatively, this can be enabled dynamically when the system detects sequential data patterns. Write Through Will not confirm completion of the write until it is written to disk. This provides assurance that there is not risk of data loss during outages, but write operations will be slower compared to write ahead. Write Back Confirms completion of write operations once written to cache. This provides much faster write operations but is less fault tolerant, since data can be lost during unexpected outages, but can be more reliable if it is backed by a battery powered RAID controller.","title":"RAID"},{"location":"storage/RAID/#raid-types","text":"RAID 0 (Striping) Stripes data across multiple disks to improve performance. Provides faster read/write speeds but offers no resiliency meaning it cannot tolerate any drive failures. RAID 1 (Mirroring) Requires a minimum of 2 drives. Creates copies of the data across the drives so the capacity of the storage is n/2 . Can withstand a single drive failure. RAID 5 (Striping with Distributed Parity) Requires a minimum of 3 drives to distribute data and parity information across. Can only withstand a single drive failure. RAID 6 (Striping with Double Parity) Distributes data across multiple drives but uses independent sets of parity data. This allows for it to withstand 2 drive failures at the same time. Requires a minimum of 4 drives. RAID 10 (Striping + Mirroring) Requires a minimum of 4 drives to create independent mirrored sets where data is striped across drives within the set. This provides the high performance of RAID 0 (still slower than RAID 5 or 6), and the fault tolerance provided by RAID 1.","title":"RAID Types"},{"location":"storage/RAID/#raid-policies","text":"","title":"RAID Policies"},{"location":"storage/RAID/#read-ahead","text":"Read sequential data ahead of time and store it in the cache to make read operations faster. Altenatively, this can be enabled dynamically when the system detects sequential data patterns.","title":"Read Ahead"},{"location":"storage/RAID/#write-through","text":"Will not confirm completion of the write until it is written to disk. This provides assurance that there is not risk of data loss during outages, but write operations will be slower compared to write ahead.","title":"Write Through"},{"location":"storage/RAID/#write-back","text":"Confirms completion of write operations once written to cache. This provides much faster write operations but is less fault tolerant, since data can be lost during unexpected outages, but can be more reliable if it is backed by a battery powered RAID controller.","title":"Write Back"},{"location":"storage/ZFS_terminology/","text":"Pooling ZFS allocates volumes into pools, so storage can be spread across multiple storage devices. ZFS removes the concept of volumes sitting between a set of storage devices and file systems. Instead it replaces it with a single storage pool. All the storage in the pool is shared. This allows for automatic growing and shrinking of the pool. Data Integrity ZFS detects and corrects data corruption. There are occasions where bits can flip within the file data which causes corruption. As far as file-system and metadata are aware, the file is fine, despite the data being corrupted. ZFS will do regular checksums to validate the data integrity and prevent these silent bit flip issues. Silent and noisy data corruption are becoming more frequent with the increase in storage size and # of devices per deployment. CERN ran a test to verify a 1GB file write/read operation against 3000 rack servers with HW RAID. After 3 weeks there were 152 instances of silent data corruption. HW RAID only detected noisy data errors. E2E Data Integrity A checksum is stored in the parent pointer and there is fault isolation between the data and the checksum. A checksum hierarchy forms a self-validating Merkle tree. ZFS will then validate the entire I/O path which helps catching silent data corruption. These checksums help detect issues in mirroring. It verifies the good data from the healthy mirrored data block, and uses this to repair the bad data on the mirrored disk. Copy On Write The atomic nature of changes are achieved via COW. When changes are made to data, it will make a copy of the initial data and only update the pointers for the portions of data that have changed. These changes need to be refelcted up the tree so the initial top-level block of the filesystem checksum will need to be updated. If there is an issue during the write process, the initial tree representing the data will still have it's pointers pointing at the initial state of the data, ensuring the atomic nature of the write process. Management Improvements ZFS is smart enough to know if a system uses big or little endian. Which means file systems can be switched between different hardware and still operate as normal. ZPL ZFS POSIX Layer is an atomic object-based transaction. The transaction will not be marked as complete until all objects/changes have been correctly stored. This ensures that if there is a power failure during changes, all the changes will be reverted to ensure a clean state on retry. DMU Data Management Unit is a transaction group commit which is atomic for an entire group. SPA Storage Pool Allocator is a Transaction Group Batch I/O which can schedule, aggregate and issue I/O at will. This part of the stack talks directly to the disks and handles where data should be written based on the current allocation of data. Constant Time Snapshots On completion of a change, a snapshot root will point at the original data. RAID-Z Dynamic stripe width with a variable block size so each logical block has its own stripe. All writes are full-stripe writes which eliminates read-modify-write and the RAID-5 write hole Detects and corrects silent data corruption. Resilvering Refers to recovering data on disk. ZFS resilvers the storage pool's block tree from teh root down, meaning the most important blocks are restored first. There is no time wasted copying free space, unlike traditional RAID, as it only copies live blocks. ZFS records the transaction group window that a device misses, and will re-walk the tree up to that point of data loss to recover from transient outages/dirty writes. Ditto Blocks Data can be replicated to update 3 physical blocks to help with recovery of corrupted data. When different devices are not available, it will be replicated to different places on the same device. Disk Scrubbing Finds latent errors while they're still correctable. It verifies the integrity of all data by traversing pool metadata to read every copy of every block (mirror copies, RAID-Z parity and ditto blocks) using its 256-bit checksum and repairs as it goes. This process has a low I/O priority to ensure that scrubbing does not interrupt day-to-day activities. Intelligent Prefetch ZFS is able to predict and prefetch the data. It will detect any linear access pattern. Variable Block Size No single block size which allows for per-object granularity so read/write times closely represent the size of the data. Zones ZFS allows for part of the pool to be separated into zones for specific use-cases","title":"ZFS terminology"},{"location":"storage/ZFS_terminology/#pooling","text":"ZFS allocates volumes into pools, so storage can be spread across multiple storage devices. ZFS removes the concept of volumes sitting between a set of storage devices and file systems. Instead it replaces it with a single storage pool. All the storage in the pool is shared. This allows for automatic growing and shrinking of the pool.","title":"Pooling"},{"location":"storage/ZFS_terminology/#data-integrity","text":"ZFS detects and corrects data corruption. There are occasions where bits can flip within the file data which causes corruption. As far as file-system and metadata are aware, the file is fine, despite the data being corrupted. ZFS will do regular checksums to validate the data integrity and prevent these silent bit flip issues. Silent and noisy data corruption are becoming more frequent with the increase in storage size and # of devices per deployment. CERN ran a test to verify a 1GB file write/read operation against 3000 rack servers with HW RAID. After 3 weeks there were 152 instances of silent data corruption. HW RAID only detected noisy data errors.","title":"Data Integrity"},{"location":"storage/ZFS_terminology/#e2e-data-integrity","text":"A checksum is stored in the parent pointer and there is fault isolation between the data and the checksum. A checksum hierarchy forms a self-validating Merkle tree. ZFS will then validate the entire I/O path which helps catching silent data corruption. These checksums help detect issues in mirroring. It verifies the good data from the healthy mirrored data block, and uses this to repair the bad data on the mirrored disk.","title":"E2E Data Integrity"},{"location":"storage/ZFS_terminology/#copy-on-write","text":"The atomic nature of changes are achieved via COW. When changes are made to data, it will make a copy of the initial data and only update the pointers for the portions of data that have changed. These changes need to be refelcted up the tree so the initial top-level block of the filesystem checksum will need to be updated. If there is an issue during the write process, the initial tree representing the data will still have it's pointers pointing at the initial state of the data, ensuring the atomic nature of the write process.","title":"Copy On Write"},{"location":"storage/ZFS_terminology/#management-improvements","text":"ZFS is smart enough to know if a system uses big or little endian. Which means file systems can be switched between different hardware and still operate as normal.","title":"Management Improvements"},{"location":"storage/ZFS_terminology/#zpl","text":"ZFS POSIX Layer is an atomic object-based transaction. The transaction will not be marked as complete until all objects/changes have been correctly stored. This ensures that if there is a power failure during changes, all the changes will be reverted to ensure a clean state on retry.","title":"ZPL"},{"location":"storage/ZFS_terminology/#dmu","text":"Data Management Unit is a transaction group commit which is atomic for an entire group.","title":"DMU"},{"location":"storage/ZFS_terminology/#spa","text":"Storage Pool Allocator is a Transaction Group Batch I/O which can schedule, aggregate and issue I/O at will. This part of the stack talks directly to the disks and handles where data should be written based on the current allocation of data.","title":"SPA"},{"location":"storage/ZFS_terminology/#constant-time-snapshots","text":"On completion of a change, a snapshot root will point at the original data.","title":"Constant Time Snapshots"},{"location":"storage/ZFS_terminology/#raid-z","text":"Dynamic stripe width with a variable block size so each logical block has its own stripe. All writes are full-stripe writes which eliminates read-modify-write and the RAID-5 write hole Detects and corrects silent data corruption.","title":"RAID-Z"},{"location":"storage/ZFS_terminology/#resilvering","text":"Refers to recovering data on disk. ZFS resilvers the storage pool's block tree from teh root down, meaning the most important blocks are restored first. There is no time wasted copying free space, unlike traditional RAID, as it only copies live blocks. ZFS records the transaction group window that a device misses, and will re-walk the tree up to that point of data loss to recover from transient outages/dirty writes.","title":"Resilvering"},{"location":"storage/ZFS_terminology/#ditto-blocks","text":"Data can be replicated to update 3 physical blocks to help with recovery of corrupted data. When different devices are not available, it will be replicated to different places on the same device.","title":"Ditto Blocks"},{"location":"storage/ZFS_terminology/#disk-scrubbing","text":"Finds latent errors while they're still correctable. It verifies the integrity of all data by traversing pool metadata to read every copy of every block (mirror copies, RAID-Z parity and ditto blocks) using its 256-bit checksum and repairs as it goes. This process has a low I/O priority to ensure that scrubbing does not interrupt day-to-day activities.","title":"Disk Scrubbing"},{"location":"storage/ZFS_terminology/#intelligent-prefetch","text":"ZFS is able to predict and prefetch the data. It will detect any linear access pattern.","title":"Intelligent Prefetch"},{"location":"storage/ZFS_terminology/#variable-block-size","text":"No single block size which allows for per-object granularity so read/write times closely represent the size of the data.","title":"Variable Block Size"},{"location":"storage/ZFS_terminology/#zones","text":"ZFS allows for part of the pool to be separated into zones for specific use-cases","title":"Zones"},{"location":"storage/ceph_deployment/","text":"Host Setup For a Ceph cluster you require a minimum of 3 hosts. This setup was done using Rocky Linux 9 so commands used here will only work with a CentOS distro. The primary host will need to have SSH access to all other hosts in the Ceph cluster. All hosts need to have podman and lvm2 installed: dnf install -y podman lvm2 From the primary host, install cephadm dnf search release-ceph dnf install --assumeyes centos-release-ceph-squid dnf install --assumeyes cephadm Bootstrap From the primary monitor host, run the following commands to bootstrap the Ceph cluster. MONITOR_IP needs to be set to the IP address of the host. cephadm bootstrap --mon-ip=MONITOR_IP Add Hosts to Cluster Firstly, the public key generated for the Ceph cluster needs to be shared to the other cluster hosts: ssh-copy-id -f -i /etc/ceph/ceph.pub root@$HOST_IP From the primary host, run the following commands to add a new host to the Ceph cluster: cephadm shell ceph orch host add ceph-X $HOST_IP You can verify that the host has been added with: ceph status Additionally, you can log onto the new host, and confirm that podman is running Ceph containers: podman ps This registers the hosts monitors and backup managers, but not OSD hosts. Without OSDs, Ceph will report no available storage. Object Storage Daemon The Ceph OSD is deployed on Ceph cluster hosts to manage block storage devices. The below command will show what devices are available from what hosts: ceph orch device ls --refresh ## ######################################################################################################################################################## HOST PATH TYPE DEVICE ID SIZE AVAILABLE REFRESHED REJECT REASONS ceph-1 /dev/sdb hdd QEMU_QEMU_HARDDISK_6e7a5cd8-16de-43ef-a8c4-b737064e8f59 6144M Yes 3m ago ceph-1 /dev/sr0 ssd QEMU_DVD-ROM_QM00003 474k No 3m ago Has a FileSystem, Insufficient space (<5GB) ceph-2 /dev/sdb hdd QEMU_QEMU_HARDDISK_78d6c800-aad6-45a0-91ca-8ee23ac569ce 6144M Yes 3m ago ceph-2 /dev/sr0 hdd QEMU_DVD-ROM_QM00003 474k No 3m ago Has a FileSystem, Insufficient space (<5GB) ceph-3 /dev/sdb hdd QEMU_QEMU_HARDDISK_06803200-1fc8-4a10-8f33-e36b58145dee 6144M Yes 111s ago ceph-3 /dev/sr0 hdd QEMU_DVD-ROM_QM00003 474k No 111s ago Has a FileSystem, Insufficient space (<5GB) Providing a storage device is >5GB, has no partitions or LVM state, then it can be considered available. We can then deploy OSDs onto these hosts to manage these devices with the command below: ceph orch apply osd --all-available-devices # Alternatively target specific device ceph orch daemon add osd ceph-1:/dev/sdb You can be even more granular with the OSD specifications using the Ceph orchestrator. Below is an example of a yaml file which specifies to use spinning disks for the OSD and the Bluestore database to be configured on SSD. service_type : osd service_id : osd_all placement : host_pattern : ceph-0 data_devices : rotational : 1 db_devices : rotational : 0 This will deploy a new orch service which A new OSD daemon conatiner will be visible under the podman running containers. Once they have registered, the storage should be visible on the cluster: [ ceph: root@ceph-1 / ] # ceph status cluster : id : a1d4a086 - 502 a - 11 f0 - b3b0 - fa163e27aeff health : HEALTH_OK services : mon : 3 daemons , quorum ceph - 1 , ceph - 3 , ceph - 2 ( age 87 s ) mgr : ceph - 1. cksalp ( active , since 10 m ), standbys : ceph - 3. cjvulx osd : 3 osds : 3 up ( since 16 s ), 3 in ( since 52 s ) data : pools : 1 pools , 1 pgs objects : 2 objects , 577 KiB usage : 81 MiB used , 18 GiB / 18 GiB avail pgs : 1 active + clean","title":"Ceph deployment"},{"location":"storage/ceph_deployment/#host-setup","text":"For a Ceph cluster you require a minimum of 3 hosts. This setup was done using Rocky Linux 9 so commands used here will only work with a CentOS distro. The primary host will need to have SSH access to all other hosts in the Ceph cluster. All hosts need to have podman and lvm2 installed: dnf install -y podman lvm2 From the primary host, install cephadm dnf search release-ceph dnf install --assumeyes centos-release-ceph-squid dnf install --assumeyes cephadm","title":"Host Setup"},{"location":"storage/ceph_deployment/#bootstrap","text":"From the primary monitor host, run the following commands to bootstrap the Ceph cluster. MONITOR_IP needs to be set to the IP address of the host. cephadm bootstrap --mon-ip=MONITOR_IP","title":"Bootstrap"},{"location":"storage/ceph_deployment/#add-hosts-to-cluster","text":"Firstly, the public key generated for the Ceph cluster needs to be shared to the other cluster hosts: ssh-copy-id -f -i /etc/ceph/ceph.pub root@$HOST_IP From the primary host, run the following commands to add a new host to the Ceph cluster: cephadm shell ceph orch host add ceph-X $HOST_IP You can verify that the host has been added with: ceph status Additionally, you can log onto the new host, and confirm that podman is running Ceph containers: podman ps This registers the hosts monitors and backup managers, but not OSD hosts. Without OSDs, Ceph will report no available storage.","title":"Add Hosts to Cluster"},{"location":"storage/ceph_deployment/#object-storage-daemon","text":"The Ceph OSD is deployed on Ceph cluster hosts to manage block storage devices. The below command will show what devices are available from what hosts: ceph orch device ls --refresh ## ######################################################################################################################################################## HOST PATH TYPE DEVICE ID SIZE AVAILABLE REFRESHED REJECT REASONS ceph-1 /dev/sdb hdd QEMU_QEMU_HARDDISK_6e7a5cd8-16de-43ef-a8c4-b737064e8f59 6144M Yes 3m ago ceph-1 /dev/sr0 ssd QEMU_DVD-ROM_QM00003 474k No 3m ago Has a FileSystem, Insufficient space (<5GB) ceph-2 /dev/sdb hdd QEMU_QEMU_HARDDISK_78d6c800-aad6-45a0-91ca-8ee23ac569ce 6144M Yes 3m ago ceph-2 /dev/sr0 hdd QEMU_DVD-ROM_QM00003 474k No 3m ago Has a FileSystem, Insufficient space (<5GB) ceph-3 /dev/sdb hdd QEMU_QEMU_HARDDISK_06803200-1fc8-4a10-8f33-e36b58145dee 6144M Yes 111s ago ceph-3 /dev/sr0 hdd QEMU_DVD-ROM_QM00003 474k No 111s ago Has a FileSystem, Insufficient space (<5GB) Providing a storage device is >5GB, has no partitions or LVM state, then it can be considered available. We can then deploy OSDs onto these hosts to manage these devices with the command below: ceph orch apply osd --all-available-devices # Alternatively target specific device ceph orch daemon add osd ceph-1:/dev/sdb You can be even more granular with the OSD specifications using the Ceph orchestrator. Below is an example of a yaml file which specifies to use spinning disks for the OSD and the Bluestore database to be configured on SSD. service_type : osd service_id : osd_all placement : host_pattern : ceph-0 data_devices : rotational : 1 db_devices : rotational : 0 This will deploy a new orch service which A new OSD daemon conatiner will be visible under the podman running containers. Once they have registered, the storage should be visible on the cluster: [ ceph: root@ceph-1 / ] # ceph status cluster : id : a1d4a086 - 502 a - 11 f0 - b3b0 - fa163e27aeff health : HEALTH_OK services : mon : 3 daemons , quorum ceph - 1 , ceph - 3 , ceph - 2 ( age 87 s ) mgr : ceph - 1. cksalp ( active , since 10 m ), standbys : ceph - 3. cjvulx osd : 3 osds : 3 up ( since 16 s ), 3 in ( since 52 s ) data : pools : 1 pools , 1 pgs objects : 2 objects , 577 KiB usage : 81 MiB used , 18 GiB / 18 GiB avail pgs : 1 active + clean","title":"Object Storage Daemon"},{"location":"storage/ceph_storage_systems/","text":"Ceph can provide storage as block, object or file FileSystem Block A Ceph block device can be created with 2 commands: ceph osd pool create POOL_NAME rbd pool init POOL_NAME Block devices can be integrated with various tools such as Openstack and Kubernetes to provide backend block storage to VMs or PV storage via the Ceph CSI respectively. File System Ceph storage can also be overlayed with a filesystem using CephFS which can then be mounted.","title":"Ceph storage systems"},{"location":"storage/ceph_storage_systems/#block","text":"A Ceph block device can be created with 2 commands: ceph osd pool create POOL_NAME rbd pool init POOL_NAME Block devices can be integrated with various tools such as Openstack and Kubernetes to provide backend block storage to VMs or PV storage via the Ceph CSI respectively.","title":"Block"},{"location":"storage/ceph_storage_systems/#file-system","text":"Ceph storage can also be overlayed with a filesystem using CephFS which can then be mounted.","title":"File System"},{"location":"storage/ceph_terminology/","text":"CRUSH CRUSH controls where and how the data is stored in a Ceph cluster. It also enables mass scalability to distributing the work across the clients in the cluster. A Ceph client will be running LIBRADOS which will run the object through CRUSH. This produces the destination OSDs for the data to be stored. CRUSH rules are assigned to a pool so objects stored as part of that pool are allocated specific OSDs to be stored on. The CRUSH rules assigned to specific OSDs can be viewed with one of the following commands: ceph osd crush rule ls ceph osd pool ls detail ceph osd pool get POOL_NAME crush_rule Placement Groups Ceph can allocate OSDs to a \"pool\" which can be allocated for specific types of data. When pools are created, it also creates a set of PGs. PGs are identified with the format of POOL_ID.PG_ID where the PG ID is a hexidecimal number. Each of these PGs map to a random set of OSDs which can be viewed by running the command ceph pg map $PG_ID . Data is always stored as an object, so the file is split into objects to be stored into placement groups. The overall object is assigned to a pool of disks, whereas the split up objects are assigned placement groups which are assigned to specific disks. The placement group ensures that the replicated data is not stored on the same disk. Further crush rules can be configured to ensure replicas are stored on different hosts in the cluster, or different racks in the datacenter. It is possible to see th replicas of a placement group with the below command: ceph pg map $PG_ID Placement groups should preferably be in an active and clean state. The state can checked with: ceph pg stat Protection Mechanisms Replication Replication will replicate an object X times. It will be sent to 3 different placement groups on 3 different OSDs. When sending the data, the object will only be sent once (to th primary OSD). The primary OSD will then replicate the file accordingly to other OSDs. The maximum usable storage of a Ceph cluster will depend on the replication configuration. If replication is set to 3 on a 100TB cluster, only 33.3TB will be usable. Erasure Encoding A file is split into X chunks and is appended with Y encoding chunks. Both of these sets of chunks are written to OSDs. The Y value is recommended to be 2 or more to prevent data loss, this provides redundancy in the event of a OSD failure. There is a lot more overhead for writing and reading this type of data, but much more space efficient. Pools can be configured with ECP with the following command: ceph osd pool create NAME erasure The Erasure code profile can be seen with: ceph osd erasure-code-profile get default To understand the overhead associated with the k/m values of the profile, see the ceph docs: https://docs.ceph.com/en/reef/rados/operations/erasure-code/#erasure-coded-pool-overhead Applications Applications refer to the client usage of a Ceph pool. You can enable specific applications on pools such as object storage or block storage. # Enable block storage on ceph pool ceph osd pool application enable $POOL_NAME rbd CephFS and MDS MDS is the filesystem daemon required to be running for cephfs to work. It stands for Metadata Server. CephFS setups require a metadata pool and a data pool when being configured RGW RGW stands for Rados Gateway.","title":"Ceph terminology"},{"location":"storage/ceph_terminology/#crush","text":"CRUSH controls where and how the data is stored in a Ceph cluster. It also enables mass scalability to distributing the work across the clients in the cluster. A Ceph client will be running LIBRADOS which will run the object through CRUSH. This produces the destination OSDs for the data to be stored. CRUSH rules are assigned to a pool so objects stored as part of that pool are allocated specific OSDs to be stored on. The CRUSH rules assigned to specific OSDs can be viewed with one of the following commands: ceph osd crush rule ls ceph osd pool ls detail ceph osd pool get POOL_NAME crush_rule","title":"CRUSH"},{"location":"storage/ceph_terminology/#placement-groups","text":"Ceph can allocate OSDs to a \"pool\" which can be allocated for specific types of data. When pools are created, it also creates a set of PGs. PGs are identified with the format of POOL_ID.PG_ID where the PG ID is a hexidecimal number. Each of these PGs map to a random set of OSDs which can be viewed by running the command ceph pg map $PG_ID . Data is always stored as an object, so the file is split into objects to be stored into placement groups. The overall object is assigned to a pool of disks, whereas the split up objects are assigned placement groups which are assigned to specific disks. The placement group ensures that the replicated data is not stored on the same disk. Further crush rules can be configured to ensure replicas are stored on different hosts in the cluster, or different racks in the datacenter. It is possible to see th replicas of a placement group with the below command: ceph pg map $PG_ID Placement groups should preferably be in an active and clean state. The state can checked with: ceph pg stat","title":"Placement Groups"},{"location":"storage/ceph_terminology/#protection-mechanisms","text":"","title":"Protection Mechanisms"},{"location":"storage/ceph_terminology/#replication","text":"Replication will replicate an object X times. It will be sent to 3 different placement groups on 3 different OSDs. When sending the data, the object will only be sent once (to th primary OSD). The primary OSD will then replicate the file accordingly to other OSDs. The maximum usable storage of a Ceph cluster will depend on the replication configuration. If replication is set to 3 on a 100TB cluster, only 33.3TB will be usable.","title":"Replication"},{"location":"storage/ceph_terminology/#erasure-encoding","text":"A file is split into X chunks and is appended with Y encoding chunks. Both of these sets of chunks are written to OSDs. The Y value is recommended to be 2 or more to prevent data loss, this provides redundancy in the event of a OSD failure. There is a lot more overhead for writing and reading this type of data, but much more space efficient. Pools can be configured with ECP with the following command: ceph osd pool create NAME erasure The Erasure code profile can be seen with: ceph osd erasure-code-profile get default To understand the overhead associated with the k/m values of the profile, see the ceph docs: https://docs.ceph.com/en/reef/rados/operations/erasure-code/#erasure-coded-pool-overhead","title":"Erasure Encoding"},{"location":"storage/ceph_terminology/#applications","text":"Applications refer to the client usage of a Ceph pool. You can enable specific applications on pools such as object storage or block storage. # Enable block storage on ceph pool ceph osd pool application enable $POOL_NAME rbd","title":"Applications"},{"location":"storage/ceph_terminology/#cephfs-and-mds","text":"MDS is the filesystem daemon required to be running for cephfs to work. It stands for Metadata Server. CephFS setups require a metadata pool and a data pool when being configured","title":"CephFS and MDS"},{"location":"storage/ceph_terminology/#rgw","text":"RGW stands for Rados Gateway.","title":"RGW"},{"location":"storage/linux_storage_commands/","text":"# View storage consumption of disk / mounts df - h # View block storage devices lsblk # Generate a file of particular size # Reads from / dev / zero to generate a 0 . 5 GB file full of 0 s dd if =/ dev / zero of = NAME bs = 512 M count = 1 # Check storage consumption recusively throughout a path ncdu - x $ PATH","title":"Linux storage commands"},{"location":"storage/lustre_overview/","text":"Benefits of Lustre Lustre is a storage system primarily used by HPC due to its high-bandwidth storage access. Lustre stripes data across multiple OSTs (Object Storage Targets). When accessing these OSTs, it can be done in parallel which helps reduce IO strain on a single machine.","title":"Lustre overview"},{"location":"storage/lustre_overview/#benefits-of-lustre","text":"Lustre is a storage system primarily used by HPC due to its high-bandwidth storage access. Lustre stripes data across multiple OSTs (Object Storage Targets). When accessing these OSTs, it can be done in parallel which helps reduce IO strain on a single machine.","title":"Benefits of Lustre"},{"location":"storage/minio_kube_auth/","text":"STS Authentication Minio Security Token Service provides a way of authenticating with Minio tenants without needing to create authentication credentials on the tenant. In this setup, we will be using Kubernetes service accounts with the AssumeRoleWithWebIdentity action for acquiring Minio access credentials. Minio Setup We will be using the Minio helm chart to deploy a tenant for testing the STS authantication. Add the Minio Helm repository: helm repo add minio https://operator.min.io helm repo update Install the Minio Operator: helm install operator minio/operator -n minio-operator --create-namespace The operator allows us to provision multiple Minio tenants using Minio CRDs. We can now deploy a Minio tenant once the operator pods are up and running: helm install tenant minio/tenant -n minio-tenant --create-namespace This will deploy a tenant custom resource which can be checked with kubectl get tenants . The tenants will need to be configured with TLS enabled. By default the Helm chart configures the tenants with \"auto-cert\", which will use the Kubernetes CA cert on the cluster for generating TLS certificates for communication between the client, tenant and the Minio operator. This can be disabled and certs can be managed manually by configurating the chart valies with certificate.requestAutoCert=false . There are Minio STS deployment examples here on how to configure certificates with CertManager: https://github.com/minio/operator/tree/master/examples/kustomization/sts-example. Service Account Setup The token that will be passed to the STS endpoint for authentication will be the JWT tied to a Kubernetes service account. The yaml below will create a service account in the cluster which will be assigned to a client service. apiVersion : v1 kind : ServiceAccount metadata : name : minio-client-sa When assigning the service account to the client deployment, the sts.min.io audience needs to be added to the token, which can be achieved using projected volume mounts: apiVersion : apps/v1 kind : Deployment metadata : name : minio-client spec : template : spec : containers : - name : minio-client image : $IMAGE name : fridge-api-server ports : - containerPort : 8000 protocol : TCP volumeMounts : - mountPath : /minio name : minio-client-sa readOnly : true serviceAccountName : minio-client-sa volumes : - name : minio-client-sa projected : sources : - serviceAccountToken : audience : sts.min.io path : token The service account token with the projected audience will be mounted at /minio/token . Policy Binding The service account needs to be bound to a Minio policy. This can be achieved using the Minio PolicyBinding CRD: apiVersion : sts.min.io/v1beta1 kind : PolicyBinding metadata : name : minio-client-readonly spec : application : namespace : client # Namespace of the client using the service account serviceaccount : minio-client-sa # Service account name policies : - readonly Python Client Minio STS requires TLS encryption to work. Since the certificates used by the Minio tenant and operator are issued by the cluster CA, the CA cert needs to be trusted within the client. The certificates are also only valid for internal cluster URLs (e.g. svc.cluster.local ), meaning the client needs to be running from somewhere where that can be resolved. This certificate is already accessible within the pod when a Kubernetes service account is assigned to the pod. The cert can be found in the pod under /var/run/secrets/kubernetes.io/serviceaccount/ca.crt The Python script below assumes that it is running within the cluster, with the internal Kube DNS to resolve svc.cluster.local . It passes the cluster certificate into the SSL context for the HTTP call, which calls the /sts/$TENANT Minio operator STS endpoint passing the Action=AssumeRoleWithWebIdentity and WebIdentityToken=$JWT . import ssl import urllib3 from pathlib import Path SA_TOKEN_FILE = \"/minio/token\" KUBE_CA_CRT = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\" STS_ENDPOINT = \"https://sts.minio-operator.svc.cluster.local:4223\" TENANT = \"minio-tenant\" # Read service account token sa_token = Path ( SA_TOKEN_FILE ) . read_text () . strip () ssl_context = ssl . create_default_context ( cafile = KUBE_CA_CRT ) # Create urllib3 client which accepts kube CA cert http = urllib3 . PoolManager ( ssl_context = ssl_context ) # Send the token to the MinIO STS endpoint response = http . request ( \"POST\" , f \" { STS_ENDPOINT } /sts/ { TENANT } ?Action=AssumeRoleWithWebIdentity&Version=2011-06-15&WebIdentityToken= { sa_token } \" , ) print ( response . data ) Providing all went to plan, Minio should return a block of XML containing the access and secret key, looking similar to the block below. <AssumeRoleWithWebIdentityResponse xmlns= \"https://sts.amazonaws.com/doc/2011-06-15/\" > <AssumeRoleWithWebIdentityResult> <AssumedRoleUser> <Arn></Arn> <AssumeRoleId></AssumeRoleId> </AssumedRoleUser> <Credentials> <AccessKeyId> ACCESS_KEY </AccessKeyId> <SecretAccessKey> SECRET_KET </SecretAccessKey> <Expiration> 2020-12-25T00:00:50Z </Expiration> <SessionToken> JWT </SessionToken> </Credentials> </AssumeRoleWithWebIdentityResult> <ResponseMetadata></ResponseMetadata> </AssumeRoleWithWebIdentityResponse> This XML can then be parsed to grab the keys and configure the minio Python client. from minio import Minio minio_client = Minio ( \"minio.minio-tenant.svc.cluster.local:9000\" , access_key = $ ACCESS_KEY , secret_key = $ SECRET_KEY , secure = True , )","title":"Minio kube auth"},{"location":"storage/minio_kube_auth/#sts-authentication","text":"Minio Security Token Service provides a way of authenticating with Minio tenants without needing to create authentication credentials on the tenant. In this setup, we will be using Kubernetes service accounts with the AssumeRoleWithWebIdentity action for acquiring Minio access credentials.","title":"STS Authentication"},{"location":"storage/minio_kube_auth/#minio-setup","text":"We will be using the Minio helm chart to deploy a tenant for testing the STS authantication. Add the Minio Helm repository: helm repo add minio https://operator.min.io helm repo update Install the Minio Operator: helm install operator minio/operator -n minio-operator --create-namespace The operator allows us to provision multiple Minio tenants using Minio CRDs. We can now deploy a Minio tenant once the operator pods are up and running: helm install tenant minio/tenant -n minio-tenant --create-namespace This will deploy a tenant custom resource which can be checked with kubectl get tenants . The tenants will need to be configured with TLS enabled. By default the Helm chart configures the tenants with \"auto-cert\", which will use the Kubernetes CA cert on the cluster for generating TLS certificates for communication between the client, tenant and the Minio operator. This can be disabled and certs can be managed manually by configurating the chart valies with certificate.requestAutoCert=false . There are Minio STS deployment examples here on how to configure certificates with CertManager: https://github.com/minio/operator/tree/master/examples/kustomization/sts-example.","title":"Minio Setup"},{"location":"storage/minio_kube_auth/#service-account-setup","text":"The token that will be passed to the STS endpoint for authentication will be the JWT tied to a Kubernetes service account. The yaml below will create a service account in the cluster which will be assigned to a client service. apiVersion : v1 kind : ServiceAccount metadata : name : minio-client-sa When assigning the service account to the client deployment, the sts.min.io audience needs to be added to the token, which can be achieved using projected volume mounts: apiVersion : apps/v1 kind : Deployment metadata : name : minio-client spec : template : spec : containers : - name : minio-client image : $IMAGE name : fridge-api-server ports : - containerPort : 8000 protocol : TCP volumeMounts : - mountPath : /minio name : minio-client-sa readOnly : true serviceAccountName : minio-client-sa volumes : - name : minio-client-sa projected : sources : - serviceAccountToken : audience : sts.min.io path : token The service account token with the projected audience will be mounted at /minio/token .","title":"Service Account Setup"},{"location":"storage/minio_kube_auth/#policy-binding","text":"The service account needs to be bound to a Minio policy. This can be achieved using the Minio PolicyBinding CRD: apiVersion : sts.min.io/v1beta1 kind : PolicyBinding metadata : name : minio-client-readonly spec : application : namespace : client # Namespace of the client using the service account serviceaccount : minio-client-sa # Service account name policies : - readonly","title":"Policy Binding"},{"location":"storage/minio_kube_auth/#python-client","text":"Minio STS requires TLS encryption to work. Since the certificates used by the Minio tenant and operator are issued by the cluster CA, the CA cert needs to be trusted within the client. The certificates are also only valid for internal cluster URLs (e.g. svc.cluster.local ), meaning the client needs to be running from somewhere where that can be resolved. This certificate is already accessible within the pod when a Kubernetes service account is assigned to the pod. The cert can be found in the pod under /var/run/secrets/kubernetes.io/serviceaccount/ca.crt The Python script below assumes that it is running within the cluster, with the internal Kube DNS to resolve svc.cluster.local . It passes the cluster certificate into the SSL context for the HTTP call, which calls the /sts/$TENANT Minio operator STS endpoint passing the Action=AssumeRoleWithWebIdentity and WebIdentityToken=$JWT . import ssl import urllib3 from pathlib import Path SA_TOKEN_FILE = \"/minio/token\" KUBE_CA_CRT = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\" STS_ENDPOINT = \"https://sts.minio-operator.svc.cluster.local:4223\" TENANT = \"minio-tenant\" # Read service account token sa_token = Path ( SA_TOKEN_FILE ) . read_text () . strip () ssl_context = ssl . create_default_context ( cafile = KUBE_CA_CRT ) # Create urllib3 client which accepts kube CA cert http = urllib3 . PoolManager ( ssl_context = ssl_context ) # Send the token to the MinIO STS endpoint response = http . request ( \"POST\" , f \" { STS_ENDPOINT } /sts/ { TENANT } ?Action=AssumeRoleWithWebIdentity&Version=2011-06-15&WebIdentityToken= { sa_token } \" , ) print ( response . data ) Providing all went to plan, Minio should return a block of XML containing the access and secret key, looking similar to the block below. <AssumeRoleWithWebIdentityResponse xmlns= \"https://sts.amazonaws.com/doc/2011-06-15/\" > <AssumeRoleWithWebIdentityResult> <AssumedRoleUser> <Arn></Arn> <AssumeRoleId></AssumeRoleId> </AssumedRoleUser> <Credentials> <AccessKeyId> ACCESS_KEY </AccessKeyId> <SecretAccessKey> SECRET_KET </SecretAccessKey> <Expiration> 2020-12-25T00:00:50Z </Expiration> <SessionToken> JWT </SessionToken> </Credentials> </AssumeRoleWithWebIdentityResult> <ResponseMetadata></ResponseMetadata> </AssumeRoleWithWebIdentityResponse> This XML can then be parsed to grab the keys and configure the minio Python client. from minio import Minio minio_client = Minio ( \"minio.minio-tenant.svc.cluster.local:9000\" , access_key = $ ACCESS_KEY , secret_key = $ SECRET_KEY , secure = True , )","title":"Python Client"}]}